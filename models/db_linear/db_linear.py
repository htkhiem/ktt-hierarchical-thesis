"""Implementation of the DistilBERT+Linear model."""
import os
import yaml
import pandas as pd
import torch
import numpy as np
from tqdm import tqdm

from models import model_pytorch
from utils.hierarchy import PerLevelHierarchy
from utils.encoders.distilbert import get_pretrained, get_tokenizer, \
    export_trained, DistilBertPreprocessor
from utils.build import init_folder_structure
from .bentoml import svc_lts

REFERENCE_SET_FEATURE_POOL = 32
POOLED_FEATURE_SIZE = 768 // REFERENCE_SET_FEATURE_POOL


class DB_Linear(model_pytorch.PyTorchModel, torch.nn.Module):
    """Wrapper class combining DistilBERT with the above module."""

    def __init__(
        self,
        hierarchy,
        config
    ):
        """Construct the DistilBERT+Linear model.

        Parameters
        ----------
        hierarchy : PerLevelHierarchy
            A `PerLevelHierarchy` instance to build the model on. The instance
            in question must have the `M`-matrix field computed.
        config : dict
            A configuration dictionary. See the corresponding docs section for
            fields used by this model.
        """
        super(DB_Linear, self).__init__()
        self.encoder = get_pretrained()
        self.classifier = torch.nn.Sequential(
                torch.nn.Dropout(p=config['dropout']),  # read the dropout hyperparam here
                torch.nn.Linear(768, hierarchy.levels[-1])  # output size = leaf layer size
        )
        self.hierarchy = hierarchy
        self.output_size = hierarchy.levels[-1]
        self.config = config
        self.device = 'cpu'
        self.pool = torch.nn.AvgPool1d(REFERENCE_SET_FEATURE_POOL)

    @classmethod
    def from_checkpoint(cls, path):
        """Construct model from saved checkpoints as produced by previous\
        instances of this model.

        Parameters
        ----------
        path : str
            Path to the checkpoint. Checkpoints have a `.pt` extension.

        Returns
        -------
        instance : DB_AC_HMCNN
            An instance that fully replicates the one producing the checkpoint.

        See also
        --------
        save : Create a checkpoint readable by this method.
        load : An alternative to this method for already-constructed instances.
        """
        if not os.path.exists(path):
            if not os.path.exists(path + '.dvc'):
                raise OSError('Checkpoint not present and cannot be retrieved')
        os.system('dvc checkout {}.dvc'.format(path))
        checkpoint = torch.load(path)
        hierarchy = PerLevelHierarchy.from_dict(checkpoint['hierarchy'])
        instance = cls(hierarchy, checkpoint['config'])
        instance.encoder.load_state_dict(
            checkpoint['encoder_state_dict']
        )
        instance.classifier.load_state_dict(
            checkpoint['classifier_state_dict']
        )
        return instance

    @classmethod
    def get_preprocessor(cls, config):
        """Return a DistilBERT preprocessor instance for this model."""
        return DistilBertPreprocessor(config)

    def forward(self, ids, mask):
        """Forward-propagate tokeniser input to generate classification.

        This model takes in the `ids` and `mask` tensors as generated by a
        `DistilBertTokenizer` or `DistilBertTokenizerFast` instance.

        Parameters
        ----------
        ids : torch.LongTensor of shape (batch_size, num_choices)
             Indices of input sequence tokens in the vocabulary.
        mask : torch.FloatTensor of shape (batch_size, num_choices), optional
            Mask to avoid performing attention on padding token indices. Mask
            values are 0 for real tokens and 1 for masked tokens such as pads.

        Returns
        -------
        scores : torch.FloatTensor of shape (batch_size, class_count)
            Classification scores within (0, 1). Classes are ordered by their
            hierarchical level. To extract the predicted classes, one can
            `argmax` ranges of the second dimension corresponding to each
            level.
        """
        return self.classifier(
            self.encoder(
                ids, attention_mask=mask
            )[0][:, 0, :]
        )

    def save(self, path, optim, dvc=True):
        """Save model state to disk using PyTorch's pickle facilities.

        The model state is saved as a `.pt` checkpoint with all the weights
        as well as supplementary data required to reconstruct this exact
        instance, including its topology. See the related docs section for
        the schema of checkpoints produced by this model.

        This method is mostly used internally but could be of use in case
        one prefers to implement a custom training routine.

        Parameters
        ----------
        path : str
             Path to save the checkpoint file to.
        optim : torch.optim.Optimizer
            The current optimiser instance. Checkpoints also save optimiser
            state for resuming training in the future.
        dvc : bool
            Whether to add this checkpoint to Data Version Control.
        """
        checkpoint = {
            'config': self.config,
            'hierarchy': self.hierarchy.to_dict(),
            'encoder_state_dict': self.encoder.state_dict(),
            'classifier_state_dict': self.classifier.state_dict(),
            'optimizer_state_dict': optim
        }
        torch.save(checkpoint, path)

        if dvc:
            os.system('dvc add ' + path)

    def load(self, path):
        """Load model state from disk.

        Unlike `from_checkpoint`, this method does not alter the instance's
        topology and as such can only be used with checkpoints whose topology
        matches exactly with this instance. In other words, if you use this
        method with a checkpoint, you have to ensure that the current instance's
        topology matches that of the past instance that saved said checkpoint.
        This method is useful for loading a previous state of the same instance
        for benchmarking or continuing training, as it not only loads weights
        but also returns the previous optimiser state.

        Parameters
        ----------
        path : str
             Path to the checkpoint file.

        Returns
        -------
        optim_dict : dict
        The state dictionary of the optimiser at that time, which can be loaded
        using `optimizer.load_state_dict()`.
        """
        if not os.path.exists(path):
            if not os.path.exists(path + '.dvc'):
                raise OSError('Checkpoint not present and cannot be retrieved')
            os.system('dvc checkout {}.dvc'.format(path))
        checkpoint = torch.load(path)
        self.encoder.load_state_dict(checkpoint['encoder_state_dict'])
        self.classifier.load_state_dict(checkpoint['classifier_state_dict'])
        return checkpoint['optimizer_state_dict']

    def fit(
            self,
            train_loader,
            val_loader,
            path=None,
            best_path=None,
            resume_from=None,
            dvc=True
    ):
        """Train this DistilBERT + Linear instance.

        Parameters
        ----------
        train_loader : torch.utils.data.DataLoader
            A minibatched, shuffled PyTorch DataLoader containing the training
            set.
        val_loader : torch.utils.data.DataLoader
            A minibatched, shuffled PyTorch DataLoader containing the validation
            set.
        path : str, optional
            Path to save the latest epoch's checkpoint to. If this or `best_path`
            is unspecified, no checkpoint will be saved (dry-run).
        best_path: str, optional
            Path to separately save the best-performing epoch's checkpoint to.
            If this or `path` is unspecified, no checkpoint will be saved
            (dry-run).
        resume_from: str, optional
            (to be implemented)
        dvc : bool
            Whether to add saved checkpoints to Data Version Control.

        Returns
        -------
        val_metrics : numpy.ndarray of size (epoch_count, 4)
            Accumulated validation set metrics over all epochs. Four metrics are
            stored: leaf-level accuracy, leaf-level precision, averaged accuracy
            and averaged precision (over all levels).
        """
        # Keep min validation (test set) loss so we can separately back up our
        # best-yet model
        val_loss_min = np.Inf

        criterion = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam([
            {
                'params': self.encoder.parameters(),
                'lr': self.config['encoder_lr']
            },
            {
                'params': self.classifier.parameters(),
                'lr': self.config['classifier_lr']
            }
        ])

        # Store validation metrics after each epoch
        val_metrics = np.empty((4, 0), dtype=float)

        for epoch in range(1, self.config['epoch'] + 1):
            val_loss = 0
            self.train()
            print('Epoch {}: Training'.format(epoch))
            for batch_idx, data in enumerate(tqdm(train_loader)):
                ids = data['ids'].to(self.device, dtype=torch.long)
                mask = data['mask'].to(self.device, dtype=torch.long)
                targets = data['labels'].to(self.device, dtype=torch.long)

                output = self.forward(ids, mask)

                optimizer.zero_grad()

                loss = criterion(output, targets[:, -1])
                loss.backward()
                optimizer.step()


            print('Epoch {}: Validating'.format(epoch))
            self.eval()

            val_targets = np.array([], dtype=float)
            val_outputs = np.empty((0, self.output_size), dtype=float)

            with torch.no_grad():
                for batch_idx, data in tqdm(enumerate(val_loader)):
                    ids = data['ids'].to(self.device,
                                         dtype=torch.long)
                    mask = data['mask'].to(self.device,
                                           dtype=torch.long)
                    targets = data['labels'].to(self.device,
                                                dtype=torch.long)

                    output = self.forward(ids, mask)

                    loss = criterion(output, targets[:, -1])

                    val_targets = np.concatenate([
                        val_targets, targets.cpu().detach().numpy()[:, -1]
                    ])
                    val_outputs = np.concatenate([
                        val_outputs, output.cpu().detach().numpy()
                    ])

                val_metrics = np.concatenate([
                    val_metrics,
                    np.expand_dims(
                        model_pytorch.get_metrics(
                            {
                                'outputs': val_outputs,
                                'targets': val_targets
                            },
                            display=None),
                        axis=1
                    )
                ], axis=1)

                if path is not None and best_path is not None:
                    optim = optimizer.state_dict()
                    self.save(path, optim, dvc)
                    if val_loss <= val_loss_min:
                        print('Validation loss decreased ({:.6f} --> {:.6f}).'
                              'Saving best model...'.format(
                                  val_loss_min, val_loss))
                        val_loss_min = val_loss
                        self.save(best_path, optim)
                print('Epoch {}: Done\n'.format(epoch))
        return val_metrics

    def test(self, loader):
        """Test this model on a dataset.

        This method can be used to run this instance (trained or not) over any
        dataset wrapped in a suitable PyTorch DataLoader. No gradient descent
        or backpropagation will take place.

        Parameters
        ----------
        loader : torch.utils.data.DataLoader
            A minibatched, shuffled PyTorch DataLoader containing the training
            set.

        Returns
        -------
        test_metrics : numpy.ndarray of size (epoch_count, 4)
            Accumulated validation set metrics over all epochs. Four metrics
            are stored: leaf-level accuracy, leaf-level precision, averaged
            accuracy and averaged precision (over all levels).
        """
        self.eval()

        all_targets = np.array([], dtype=bool)
        all_outputs = np.empty((0, self.output_size), dtype=float)

        with torch.no_grad():
            for batch_idx, data in enumerate(tqdm(loader)):
                ids = data['ids'].to(self.device, dtype=torch.long)
                mask = data['mask'].to(self.device, dtype=torch.long)
                targets = data['labels']

                output = self.forward(ids, mask)

                all_targets = np.concatenate([
                    all_targets, targets.numpy()[:, -1]
                ])
                all_outputs = np.concatenate([
                    all_outputs, output.cpu().detach().numpy()
                ])
        return {
            'targets': all_targets,
            'outputs': all_outputs,
        }

    def forward_with_features(self, ids, mask):
        encoder_outputs = self.encoder(ids, mask)[0][:, 0, :]
        local_outputs = self.classifier(
            encoder_outputs
        )
            
        return local_outputs, self.pool(encoder_outputs)

    def gen_reference_set(self, loader):
        self.eval()
        all_pooled_features = np.empty((0, POOLED_FEATURE_SIZE))
        all_targets = np.empty((0), dtype=int)
        all_outputs = np.empty(
            (0, self.hierarchy.levels[-1]), dtype=float)

        with torch.no_grad():
            for batch_idx, data in enumerate(tqdm(loader)):
                ids = data['ids'].to(self.device, dtype=torch.long)
                mask = data['mask'].to(self.device, dtype=torch.long)
                targets = data['labels']

                leaf_outputs, pooled_features = self.\
                    forward_with_features(ids, mask)
                all_pooled_features = np.concatenate(
                    [all_pooled_features, pooled_features.cpu()]
                )
                # Only store leaves
                all_targets = np.concatenate([all_targets, targets[:, -1]])
                all_outputs = np.concatenate([all_outputs, leaf_outputs.cpu()])

        cols = {
            'targets': all_targets
        }
        leaf_start = self.hierarchy.level_offsets[-2]
        for col_idx in range(all_pooled_features.shape[1]):
            cols[str(col_idx)] = all_pooled_features[:, col_idx]
        for col_idx in range(all_outputs.shape[1]):
            cols[
                self.hierarchy.classes[leaf_start + col_idx]
            ] = all_outputs[:, col_idx]
        return pd.DataFrame(cols)


    def export(
            self, dataset_name, bento=False, reference_set_path=None
    ):
        """Export model to ONNX/Bento.

        If ONNX is selected (i.e. bento=False), then both the DistilBERT
        encoder (with tokeniser) and classifier head are exported separately
        as two ONNX models. If BentoML is selected instead, they will be
        serialised using BentoML's default serialisation facilities, and
        a complete BentoService will be built.

        Parameters
        ----------
        dataset_name: str
            Name of the dataset this instance was trained on. Use the folder
            name of the intermediate version in the datasets folder.
        bento: bool
            Whether to export this model as a BentoML service or not.
        reference_set_path: str
            Path to an optional reference dataset compatible with Evidently's
            schema. It will be copied to the built service's folder. Only
            applicable when Bento exporting is selected.
            If not passed, the generated BentoService will not be configured to
            work with Evidently.
        """
        self.eval()
        # Create dummy input for tracing
        batch_size = 1  # Dummy batch size. When exported, it will be dynamic
        x = torch.randn(batch_size, 768, requires_grad=True).to(
            self.device
        )
        if not bento:
                # Export to ONNX graphs.
                # KTT provides utilities for exporting trained DistilBERT instances.
            export_trained(
                self.encoder,
                dataset_name,
                'db_linear',
            )
            # Prepare names and paths
            name = '{}_{}'.format(
                'db_linear',
                dataset_name
            )
            path = 'output/{}/classifier/'.format(name)
            if not os.path.exists(path):
                os.makedirs(path)
            path += 'classifier.onnx'
            # Clear previous versions
            if os.path.exists(path):
                os.remove(path)
            # Export into transformers model .bin format
            # Since our model is minibatched, we have to make the first axis
            # dynamic. In production, batch sizes can vary depending on load
            # (or stay at 1 if no microbatching is available).
            torch.onnx.export(
                self.classifier,
                x,
                path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            # Additionally export hierarchical metadata to the same folder.
            self.hierarchy.to_json(
                "output/{}/hierarchy.json".format(name)
            )
        else:
            # Export as BentoML service
            build_path = 'build/db_linear_' + dataset_name.lower()
            build_path_inference = ''
            if reference_set_path is not None:
                    # If a path to a reference dataset is available, export the
                    # model as a service with monitoring capabilities.
                with open(
                        'models/db_linear/bentoml/evidently.yaml', 'r'
                ) as evidently_template:
                    config = yaml.safe_load(evidently_template)
                    config['prediction'] = self.hierarchy.classes[
                        self.hierarchy.level_offsets[-2]:
                        self.hierarchy.level_offsets[-1]
                    ]
                # Init folder structure, Evidently YAML and so on.
                build_path_inference = init_folder_structure(
                    build_path,
                    {
                        'reference_set_path': reference_set_path,
                        'grafana_dashboard_path':
                            'models/db_linear/bentoml/dashboard.json',
                        'evidently_config': config
                    }
                )
            else:
                    # Init folder structure for a minimum system (no monitoring)
                build_path_inference = init_folder_structure(build_path)
            # Initialise a BentoService instance - we'll come to this soon
            svc = svc_lts.DB_Linear()
            # Pack tokeniser along with encoder. Here we use KTT's DistilBERT
            # facilities.
            encoder = {
                'tokenizer': get_tokenizer(),
                'model': self.encoder
            }
            svc.pack('encoder', encoder)
            svc.pack('classifier', torch.jit.trace(self.classifier, x))
            svc.pack('hierarchy', self.hierarchy.to_dict())
            svc.pack('config', {
                'monitoring_enabled': reference_set_path is not None
            })
            # Export the BentoService to the correct path.
            svc.save_to_dir(build_path_inference)


    def to(self, device=None):
        """
        Move this module to specified device.

        This overloads the default PT module's to() method to additionally
        set its internal device variable.

        Parameters
        ----------
        device : torch.device ,optional
            The device to move the model to.

        Returns
        -------
        self
        """
        super().to(device)
        if device is not None:
            self.device = device
        return self


if __name__ == "__main__":
    pass
