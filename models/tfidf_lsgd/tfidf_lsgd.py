"""Implementation of the tfidf + leaf SGD classifier model."""
import os
import joblib

from sklearn import preprocessing, linear_model
from sklearn.pipeline import Pipeline

from sklearn.feature_extraction.text import TfidfVectorizer
from skl2onnx import to_onnx
from skl2onnx.common.data_types import StringTensorType
import bentoml

from models import model

REFERENCE_SET_FEATURE_POOL=64

class Tfidf_LSGD(model.Model):
    """A wrapper class around the scikit-learn-based tfidf-LSGD model.

    It exposes the same method signatures as the PyTorch-based models for
    ease of use in the main training controller.
    """

    def __init__(self, config=None, verbose=False):
        """Construct the TF-IDF + LeafSGD model.

        Parameters
        ----------
        config : dict
            A configuration dictionary. See the corresponding docs section for
            fields used by this model.
        """
        clf = linear_model.SGDClassifier(
            loss=config['loss'],
            max_iter=config['max_iter']
        )
        self.pipeline = Pipeline([
            ('tfidf', TfidfVectorizer(config['min_df'])),
            ('clf', clf),
        ])
        self.config = config

    @classmethod
    def from_checkpoint(cls, path):
        """Construct model from saved checkpoints as produced by previous
        instances of this model.

        Parameters
        ----------
        path : str
            Path to the checkpoint. Checkpoints have a `.pt` extension.

        Returns
        -------
        instance : Tfidf_LSGD
            An instance that fully replicates the one producing the checkpoint.

        See also
        --------
        save : Create a checkpoint readable by this method.
        load : An alternative to this method for already-constructed instances.
        """
        instance = cls()
        instance.load(path)
        return instance

    def save(self, path, optim=None, dvc=True):
        """Serialise the pipeline into a pickle.

        The model state is saved as a `.pt` checkpoint with all the weights
        as well as its code (Scikit-learn only). See the related docs section for
        the schema of checkpoints produced by this model.

        This method is mostly used internally but could be of use in case
        one prefers to implement a custom training routine.

        Parameters
        ----------
        path : str
             Path to save the checkpoint file to.
        dvc : bool
            Whether to add saved checkpoints to Data Version Control.
        """
        joblib.dump(self.pipeline, path)

        if dvc:
            os.system('dvc add ' + path)

    def load(self, path):
        """Load saved pipeline pickle.

        Scikit-learn models also serialise their own code, so this is equivalent
        to the ``from_checkpoint`` classmethod in PyTorch models.

        Parameters
        ----------
        path : str
             Path to the checkpoint file.
        """
        if not os.path.exists(path):
            if not os.path.exists(path + '.dvc'):
                raise OSError('Checkpoint not present and cannot be retrieved')
            os.system('dvc checkout {}.dvc'.format(path))
        self.pipeline = joblib.load(path)

    def fit(
            self,
            train_loader,
            val_loader=None,  # Unused but included for signature compatibility
            path=None,
            best_path=None,
            dvc=True
    ):
        """Train this TF-IDF + LeafSGD pipeline. No validation phase

        Parameters
        ----------
        train_loader : tuple
            A data tuple generated by model_sklearn.get_loaders() to be used
            as the training set. The tuple contains a set of stemmed text as
            training input and a set of label.
        path : str, optional
            Path to save the latest epoch's checkpoint to. If this or `best_path`
            is unspecified, no checkpoint will be saved (dry-run).
        best_path: str, optional
            Path to separately save the best-performing epoch's checkpoint to.
            If this or `path` is unspecified, no checkpoint will be saved
            (dry-run).
        dvc : bool
            Whether to add saved checkpoints to Data Version Control.
        """
        self.pipeline.fit(train_loader[0], train_loader[1])
        if path is not None or best_path is not None:
            # There's no distinction between path and best_path as there is
            # no validation phase.
            self.save(path if path is not None else best_path, dvc)
        return None

    def test(self, loader, return_encodings=False):
        """Test this model on a dataset.
        This method can be used to run this instance (trained or not) over any
        dataset wrapped in a suitable test set tuple. 

        Parameters
        ----------
        loader : tuple
            A data tuple generated by model_sklearn.get_loaders() to be used
            as the testing set. The tuple contains a set of stemmed text as
            testing input and a set of label.
        return_encodings: bool
            If true, return pooled tfidf encodings along. Useful for generating
            reference datasets.

        Returns
        -------
        test_output: dict
            The output of the testing phase, containing four metrics which can be
            retrived by model_sklearn.get_metrics(): targets, targets_b (binarized label),
            predictions and scores.
        """
        y_avg = preprocessing.label_binarize(
            loader[1],
            classes=self.pipeline.classes_
        )
        tfidf_encoding = self.pipeline.steps[0].transform(loader[0])
        scores = self.pipeline.steps[1](tfidf_encoding)
        predictions = [self.pipeline.classes_[i] for i in np.argmax(scores, axis=1)]

        res = {
            'targets': loader[1],
            'targets_b': y_avg,
            'predictions': predictions,
            'scores': scores,
        }
        if return_encodings:
            # Average-pool encodings
            res['encodings'] = np.array([
                np.average(
                    tfidf_encoding[
                        :,
                        i*REFERENCE_SET_FEATURE_POOL:
                        min((i+1)*REFERENCE_SET_FEATURE_POOL, len(scores))
                    ]
                )
                for i in range(0, pooled_feature_size)
            ])
        return res

    def gen_reference_set(self, loader):
        """Generate an Evidently-compatible reference dataset.

        Due to the data drift tests' computational demands, we only record
        average-pooled features.

        Parameters
        ----------
        loader: torch.utils.data.DataLoader
            A DataLoader wrapping around a dataset to become the reference
            dataset (ideally the test set).
        Returns
        -------
        reference_set: pandas.DataFrame
            An Evidently-compatible dataset. Numerical feature column names are
            simple stringified numbers, while targets are leaf classes' string
            names.
        """
        results = self.test(loader, return_encodings=True)
        pooled_features = results['encodings']
        scores = results['scores']
        pooled_feature_size = self.pipeline.n_features_in_ // REFERENCE_SET_FEATURE_POOL
        targets = loader[1]
        scores = result['scores']
        cols = {
            'targets': targets,
        }
        for col_idx in range(pooled_features[1]):
            cols[str(col_idx)] = pooled_features[:, col_idx]
        for col_idx in range(scores.shape[1]):
            cols[self.pipeline.classes_[col_idx]] = scores[:, col_idx]
        return pd.DataFrame(cols)


    def export(
            self, dataset_name, bento=False, reference_set_path=None
    ):
        """Export model to ONNX/Bento.

        If ONNX is selected (i.e. bento=False), then the pipeline is exported
        as one ONNX graph. If BentoML is selected instead, they will be
        serialised using BentoML's default serialisation facilities, and
        a complete BentoService will be built.

        Parameters
        ----------
        dataset_name: str
            Name of the dataset this instance was trained on. Use the folder
            name of the intermediate version in the datasets folder.
        bento: bool
            Whether to export this model as a BentoML service or not.
        reference_set_path: str
            Path to an optional reference dataset compatible with Evidently's
            schema. It will be copied to the built service's folder. Only
            applicable when Bento exporting is selected.
            If not passed, the generated BentoService will not be configured to
            work with Evidently.
        """
        if not bento:
            initial_type = [('str_input', StringTensorType([None, 1]))]
            onx = to_onnx(
                self.pipeline, initial_types=initial_type, target_opset=11
            )
            # Create path
            name = '{}_{}'.format(
                'tfidf_lsgd',
                dataset_name
            )
            path = 'output/{}/classifier/'.format(name)

            if not os.path.exists(path):
                os.makedirs(path)

            path += 'classifier.onnx'

            # Clear previous versions
            if os.path.exists(path):
                os.remove(path)

            # Export
            with open(path, "wb") as f:
                f.write(onx.SerializeToString())

        else:
            # Export as BentoML service
            build_path = 'build/tfidf_lsgd_' + dataset_name.lower()
            build_path_inference = ''
            if reference_set_path is not None:
                    # If a path to a reference dataset is available, export the
                    # model as a service with monitoring capabilities.
                with open(
                        'models/tfidf_lsgd/bentoml/evidently.yaml', 'r'
                ) as evidently_template:
                    config = yaml.safe_load(evidently_template)
                    config['prediction'] = self.hierarchy.classes[
                        self.hierarchy.level_offsets[-2]:
                        self.hierarchy.level_offsets[-1]
                    ]
                # Init folder structure, Evidently YAML and so on.
                build_path_inference = init_folder_structure(
                    build_path,
                    {
                        'reference_set_path': reference_set_path,
                        'grafana_dashboard_path':
                            'models/tfidf_lsgd/bentoml/dashboard.json',
                        'evidently_config': config
                    }
                )
            else:
                    # Init folder structure for a minimum system (no monitoring)
                build_path_inference = init_folder_structure(build_path)
            # Initialise a BentoService instance
            svc = svc_lts.Tfidf_LSGD()
            svc.pack('model', self.pipeline)
            svc.pack('config', {
                'monitoring_enabled': reference_set_path is not None
            })
            # Export the BentoService to the correct path.
            svc.save_to_dir(build_path_inference)
