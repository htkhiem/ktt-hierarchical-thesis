{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers as ppb\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "    \n",
    "# def load_checkpoint(checkpoint_fpath, model):\n",
    "#     \"\"\"\n",
    "#     checkpoint_fpath: path to save checkpoint\n",
    "#     model: model that we want to load checkpoint parameters into       \n",
    "#     optimizer: optimizer we defined in previous training\n",
    "#     \"\"\"\n",
    "    \n",
    "#     encoder, classifier = model\n",
    "    \n",
    "#     # load check point\n",
    "#     checkpoint = torch.load(checkpoint_fpath)\n",
    "\n",
    "#     # initialize state_dict from checkpoint to model\n",
    "#     classifier.load_state_dict(checkpoint['classifier_state_dict'])\n",
    "#     encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "\n",
    "#     # return model, optimizer, epoch value, min validation loss \n",
    "#     return (encoder, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('weights/checkpoints-avg-Walmart_30k.parquet/0_2_best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "base_encoder = ppb.DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "base_encoder_state = base_encoder.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = base_encoder\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save_pretrained(\"onnx/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ppb.DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[  101,  2256,  2171,  2024,  1047, 10536,  2213, 11937,  2278,\n",
       "         10722,   102]], dtype=int64),\n",
       " 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from onnxruntime import ExecutionMode, InferenceSession, SessionOptions\n",
    "\n",
    "#create the tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Create the InferenceSession\n",
    "options = SessionOptions()\n",
    "options.intra_op_num_threads = 1\n",
    "options.execution_mode = ExecutionMode.ORT_SEQUENTIAL\n",
    "session = InferenceSession(\"onnx/test/test.onnx\",options)\n",
    "\n",
    "tokens = tokenizer.encode_plus(\"Our name are Khym Tac Tu\")\n",
    "tokens = {name: np.atleast_2d(value).astype(np.int64) for name,value in tokens.items()}\n",
    "\n",
    "# Run (None means get all outputs)\n",
    "output = session.run(None, tokens)\n",
    "\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[ 0.24262936,  0.15104881,  0.17318514, ...,  0.26403454,\n",
       "           0.32051635,  0.32391867],\n",
       "         [ 0.40355656,  0.04133132, -0.24977589, ...,  0.1722375 ,\n",
       "           0.8100649 , -0.22530524],\n",
       "         [ 0.35763988,  0.10383253, -0.15920484, ...,  0.02595149,\n",
       "           0.57657367, -0.26459423],\n",
       "         ...,\n",
       "         [ 0.41987297,  0.17621408,  0.94034517, ...,  0.5561317 ,\n",
       "           0.08265042,  0.37265146],\n",
       "         [ 0.5976376 , -0.22609206,  0.32603344, ...,  0.42022657,\n",
       "           0.18126191,  0.98615336],\n",
       "         [ 0.6711411 ,  0.34828502, -0.02709696, ..., -0.42503494,\n",
       "          -0.7768605 , -0.02205784]]], dtype=float32)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = data['ids'].to(device, dtype = torch.long)\n",
    "mask = data['mask'].to(device, dtype = torch.long)\n",
    "targets = data['labels']#.to(device, dtype = torch.long)\n",
    "features = encoder(ids, mask)[0][:,0,:]\n",
    "#I'm stuck here"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
