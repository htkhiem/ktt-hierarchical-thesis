version: '3.7'

volumes:
  prometheus_data:
  grafana_data:

networks:
  ktt-inference:

services:

  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus/:/etc/prometheus/
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yaml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    ports:
      - 9090:9090
    networks:
      - ktt-inference
    restart: on-failure

  grafana:
    image: grafana/grafana
    depends_on:
      - prometheus
    ports:
      - 3000:3000
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning/:/etc/grafana/provisioning/
    networks:
      - ktt-inference
    user: "472"
    restart: on-failure

  inference:
    build:
      context: ./inference
      dockerfile: ./Dockerfile
    ports:
      - "5000:5000"
    runtime: nvidia
    devices:
      - /dev/nvidia0:/dev/nvidia0
      - /dev/nvidiactl:/dev/nvidiactl
      - /dev/nvidia-modeset:/dev/nvidia-modeset
      - /dev/nvidia-uvm:/dev/nvidia-uvm
      - /dev/nvidia-uvm-tools:/dev/nvidia-uvm-tools
    environment:
      LC_ALL: "C.UTF-8"
      LANG: "C.UTF-8"
      EVIDENTLY_HOST: 'monitoring'
    networks:
      - ktt-inference
    restart: on-failure

  monitoring:
    build:
      context: ./monitoring
      dockerfile: ./Dockerfile
    ports:
      - "6000:6000"
    networks:
      - ktt-inference
    restart: on-failure
