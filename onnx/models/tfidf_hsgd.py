"""Implementation of the tfidf + hierarchical SGD classifier model."""
import os
import joblib

from sklearn import preprocessing, linear_model
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn_hierarchical_classification.classifier import HierarchicalClassifier
from sklearn.feature_extraction.text import TfidfVectorizer

import bentoml

from models import model


class Tfidf_HSGD(model.Model):
    """A wrapper class around the scikit-learn-based tfidf-HSGD model.

    It exposes the same method signatures as the PyTorch-based models for
    ease of use in the main training controller.
    """

    def __init__(self, config=None, hierarchy=None, verbose=False):
        """Construct the TF-IDF + HierSGD model.

        Parameters
        ----------
        config : dict
            A configuration dictionary. See the corresponding docs section for
            fields used by this model.
        hierarchy : dict
            A special hierarchy data structure constructed by model_sklearn.make_hierarchy().
        """
        bclf = make_pipeline(linear_model.SGDClassifier(
            loss='modified_huber',
            class_weight='balanced',
        ))
        clf = HierarchicalClassifier(
            base_estimator=bclf,
            class_hierarchy=hierarchy,
        )
        self.pipeline = Pipeline([
            ('tfidf', TfidfVectorizer(min_df=50)),
            ('clf', clf),
        ])
        self.config = config

    @classmethod
    def from_checkpoint(cls, path):
        """Construct model from saved checkpoints as produced by previous
        instances of this model.

        Parameters
        ----------
        path : str
            Path to the checkpoint. Checkpoints have a `.pt` extension.

        Returns
        -------
        instance : Tfidf_HSGD
            An instance that fully replicates the one producing the checkpoint.

        See also
        --------
        save : Create a checkpoint readable by this method.
        load : An alternative to this method for already-constructed instances.
        """
        instance = cls()
        instance.pipeline = joblib.load(path)
        return instance

    def save(self, path, optim=None, dvc=True):
        """Serialise the pipeline into a pickle.

        The model state is saved as a `.pt` checkpoint with all the weights
        as well as supplementary data required to reconstruct this exact
        instance, including its topology. See the related docs section for
        the schema of checkpoints produced by this model.

        This method is mostly used internally but could be of use in case
        one prefers to implement a custom training routine.

        Parameters
        ----------
        path : str
             Path to save the checkpoint file to.
        """
        joblib.dump(self.pipeline, path)
        if dvc:
            os.system('dvc add ' + path)

    def load(self, path):
        """Load saved pipeline pickle.

        Unlike `from_checkpoint`, this method does not alter the instance's
        topology and as such can only be used with checkpoints whose topology
        matches exactly with this instance. In other words, if you use this
        method with a checkpoint, you have to ensure that the current instance's
        topology matches that of the past instance that saved said checkpoint.

        Parameters
        ----------
        path : str
             Path to the checkpoint file.
        """
        if not os.path.exists(path):
            if not os.path.exists(path + '.dvc'):
                raise OSError('Checkpoint not present and cannot be retrieved')
            os.system('dvc checkout {}.dvc'.format(path))
        self.pipeline = joblib.load(path)

    def fit(
            self,
            train_loader,
            val_loader=None,  # Unused but included for signature compatibility
            path=None,
            best_path=None,
            dvc=True
    ):
        """Train this TF-IDF + HierSGD pipeline. No validation phase

        Parameters
        ----------
        train_loader : tuple
            A data tuple generated by model_sklearn.get_loaders() to be used
            as the training set. The tuple contains a set of stemmed text as
            training input and a set of label.
        path : str, optional
            Path to save the latest epoch's checkpoint to. If this or `best_path`
            is unspecified, no checkpoint will be saved (dry-run).
        best_path: str, optional
            Path to separately save the best-performing epoch's checkpoint to.
            If this or `path` is unspecified, no checkpoint will be saved
            (dry-run).
        """
        self.pipeline.fit(train_loader[0], train_loader[1])
        if path is not None or best_path is not None:
            # There's no distinction between path and best_path as there is
            # no validation phase.
            self.save(path if path is not None else best_path, dvc)
        return None

    def test(self, loader):
        """Test this model on a dataset.

        This method can be used to run this instance (trained or not) over any
        dataset wrapped in a suitable test set tuple. 

        Parameters
        ----------
        loader : tuple
            A data tuple generated by model_sklearn.get_loaders() to be used
            as the testing set. The tuple contains a set of stemmed text as
            testing input and a set of label.

        Returns
        -------
        test_output: dict
            The output of the testing phase, containing four metrics which can be
            retrived by model_sklearn.get_metrics(): targets, targets_b (binarized label),
            predictions and scores.
        """
        y_avg = preprocessing.label_binarize(
            loader[1],
            classes=self.pipeline.classes_
        )
        predictions = self.pipeline.predict(loader[0])
        scores = self.pipeline.predict_proba(loader[0])

        return {
            'targets': loader[1],
            'targets_b': y_avg,
            'predictions': predictions,
            'scores': scores,
        }

    def export(self, dataset_name, bento=True):
        """Export this model to BentoML.

        Due to the usage of sklearn_hierarchical_classification, this model
        cannot be exported to ONNX format and only supports direct-to-BentoML
        exporting.
        For compatibility, the bento flag is still there but must always be set
        to True.
        Failure to do so will raise a RuntimeError.

        Parameters
        ----------
        dataset_name: string
            The name of the dataset used to train/test the model.
        bento: bool
            Bento flag, if on then the model will be exported directly
            into BentoML.
        """
        if not bento:
            raise RuntimeError('Tfidf-HSGD does not support ONNX exporting!')

        # Create path
        name = '{}_{}'.format(
            'tfidf_hsgd',
            dataset_name
        )

        # Only support BentoML scikit-learn runner
        bentoml.sklearn.save(name, self.pipeline)
