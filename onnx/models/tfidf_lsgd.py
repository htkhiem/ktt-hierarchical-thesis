"""Implementation of the tfidf + leaf SGD classifier model."""
import os
import joblib

from sklearn import preprocessing, linear_model
from sklearn.pipeline import Pipeline

from sklearn.feature_extraction.text import TfidfVectorizer
from skl2onnx import to_onnx
from skl2onnx.common.data_types import StringTensorType
import bentoml

from models import model


class Tfidf_LSGD(model.Model):
    """A wrapper class around the scikit-learn-based tfidf-LSGD model.

    It exposes the same method signatures as the PyTorch-based models for
    ease of use in the main training controller.
    """

    def __init__(self, config=None, verbose=False):
        """Construct the TF-IDF + LeafSGD model.

        Parameters
        ----------
        config : dict
            A configuration dictionary. See the corresponding docs section for
            fields used by this model.
        """
        clf = linear_model.SGDClassifier(
            loss='modified_huber',
            verbose=True,
            max_iter=1000
        )
        self.pipeline = Pipeline([
            ('tfidf', TfidfVectorizer(min_df=50)),
            ('clf', clf),
        ])
        self.config = config

    @classmethod
    def from_checkpoint(cls, path):
        """Construct model from saved checkpoints as produced by previous
        instances of this model.

        Parameters
        ----------
        path : str
            Path to the checkpoint. Checkpoints have a `.pt` extension.

        Returns
        -------
        instance : Tfidf_LSGD
            An instance that fully replicates the one producing the checkpoint.

        See also
        --------
        save : Create a checkpoint readable by this method.
        load : An alternative to this method for already-constructed instances.
        """
        instance = cls()
        instance.pipeline = joblib.load(path)
        return instance

    def save(self, path, optim=None):
        """Serialise the pipeline into a pickle.

        The model state is saved as a `.pt` checkpoint with all the weights
        as well as supplementary data required to reconstruct this exact
        instance, including its topology. See the related docs section for
        the schema of checkpoints produced by this model.

        This method is mostly used internally but could be of use in case
        one prefers to implement a custom training routine.

        Parameters
        ----------
        path : str
             Path to save the checkpoint file to.
        """
        joblib.dump(self.pipeline, path)

    def load(self, path):
        """Load saved pipeline pickle.

        Unlike `from_checkpoint`, this method does not alter the instance's
        topology and as such can only be used with checkpoints whose topology
        matches exactly with this instance. In other words, if you use this
        method with a checkpoint, you have to ensure that the current instance's
        topology matches that of the past instance that saved said checkpoint.

        Parameters
        ----------
        path : str
             Path to the checkpoint file.
        """
        self.pipeline = joblib.load(path)

    def fit(
            self,
            train_loader,
            val_loader=None,  # Unused but included for signature compatibility
            path=None,
            best_path=None
    ):
        """Train this TF-IDF + LeafSGD pipeline. No validation phase

        Parameters
        ----------
        train_loader : tuple
            A data tuple generated by model_sklearn.get_loaders() to be used
            as the training set. The tuple contains a set of stemmed text as
            training input and a set of label.
        path : str, optional
            Path to save the latest epoch's checkpoint to. If this or `best_path`
            is unspecified, no checkpoint will be saved (dry-run).
        best_path: str, optional
            Path to separately save the best-performing epoch's checkpoint to.
            If this or `path` is unspecified, no checkpoint will be saved
            (dry-run).
        """
        self.pipeline.fit(train_loader[0], train_loader[1])
        if path is not None or best_path is not None:
            # There's no distinction between path and best_path as there is
            # no validation phase.
            self.save(path if path is not None else best_path)
        return None

    def test(self, loader):
        """Test this model on a dataset.
        This method can be used to run this instance (trained or not) over any
        dataset wrapped in a suitable test set tuple. 

        Parameters
        ----------
        loader : tuple
            A data tuple generated by model_sklearn.get_loaders() to be used
            as the testing set. The tuple contains a set of stemmed text as
            testing input and a set of label.

        Returns
        -------
        test_output: dict
            The output of the testing phase, containing four metrics which can be
            retrived by model_sklearn.get_metrics(): targets, targets_b (binarized label),
            predictions and scores.
        """
        y_avg = preprocessing.label_binarize(
            loader[1],
            classes=self.pipeline.classes_
        )
        predictions = self.pipeline.predict(loader[0])
        scores = self.pipeline.predict_proba(loader[0])

        return {
            'targets': loader[1],
            'targets_b': y_avg,
            'predictions': predictions,
            'scores': scores,
        }

    def export(self, dataset_name, bento=False):
        """Export this model to BentoML.

        Parameters
        ----------
        dataset_name: string
            The name of the dataset used to train/test the model.
        bento: bool
            Bento flag, if on then the model will be exported directly
            into BentoML.
        """
        initial_type = [('str_input', StringTensorType([None, 1]))]
        onx = to_onnx(
            self.pipeline, initial_types=initial_type, target_opset=11
        )
        # Create path
        name = '{}_{}'.format(
            'tfidf_lsgd',
            dataset_name
        )
        path = 'output/{}/classifier/'.format(name)

        if not os.path.exists(path):
            os.makedirs(path)

        path += 'classifier.onnx'

        # Clear previous versions
        if os.path.exists(path):
            os.remove(path)

        # Export
        with open(path, "wb") as f:
            f.write(onx.SerializeToString())

        # Bento support
        if bento:
            bentoml.sklearn.save(name, self.pipeline)
