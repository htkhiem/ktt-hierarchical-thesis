{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37802,
     "status": "ok",
     "timestamp": 1635307768509,
     "user": {
      "displayName": "Khiêm Huỳnh Thiện",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKsMYrfMtAzOaIk6G51GUQwDS0P6dMkGSwN9B_aA=s64",
      "userId": "01825679077701569078"
     },
     "user_tz": -420
    },
    "id": "kzuN96GF0XuA",
    "outputId": "3888bf24-8177-496a-a316-b26e35e1bf70"
   },
   "outputs": [],
   "source": [
    "### Dataset configuration\n",
    "# The parquet folder. It should be located inside datasets/.\n",
    "DATASET_NAME   = 'Walmart_30k.parquet'\n",
    "# The input text column\n",
    "TEXT_COL_NAME  = 'title'\n",
    "# Which column to use as labelled classes. It should be a column of lists of strings.\n",
    "CLASS_COL_NAME = 'category'\n",
    "# How many hierarchical levels to work on. Note that the dataset must also have at least this many levels for every example.\n",
    "DEPTH = 2\n",
    "\n",
    "### Checkpoint configuration\n",
    "# Whether to train from scratch or to load a checkpoint\n",
    "TRAIN_FROM_SCRATCH=True\n",
    "# If training from scratch, train this many times before averaging test set results. Train/val/test split is kept static.\n",
    "TRAIN_REPEATS = 5\n",
    "# Checkpoint iteration to load if not training from scratch\n",
    "LOAD_ITERATION=1\n",
    "# Last or best results from that iteration?\n",
    "LOAD_BEST=True\n",
    "\n",
    "### System configuration\n",
    "# Will try to use your NVIDIA GPU if one is available. Set to False to force CPU computation\n",
    "PREFER_GPU         = True\n",
    "# If you don't have the huggingface transformers library installed, flip this to True.\n",
    "# You only need to do this once. Once DistilBERT has been downloaded, it will be cached in your system's default user cache folder.\n",
    "# Once it is cached, please set this to False to avoid redownloads.\n",
    "INSTALL_DISTILBERT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9r_iHMmx75DM"
   },
   "source": [
    "# Import common libraries\n",
    "And also set up a few things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shaFdRkD74o1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set up GPU if available\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() and PREFER_GPU else 'cpu'\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import logging\n",
    "\n",
    "print('Using {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTb90c5tRJsf"
   },
   "source": [
    "# Import data\n",
    "Here we'll finally be using the randomly-sampled 750k-row subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13707,
     "status": "ok",
     "timestamp": 1635307806856,
     "user": {
      "displayName": "Khiêm Huỳnh Thiện",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKsMYrfMtAzOaIk6G51GUQwDS0P6dMkGSwN9B_aA=s64",
      "userId": "01825679077701569078"
     },
     "user_tz": -420
    },
    "id": "O8e3fhFSRPPw",
    "outputId": "f62876fc-11c1-4a35-8534-aa4e27db8d07"
   },
   "outputs": [],
   "source": [
    "data = pd.read_parquet('../../datasets/{}'.format(DATASET_NAME))\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print (data.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96eLOv0hMvCv"
   },
   "source": [
    "# Categorical-encode the categories\n",
    "For this notebook, we'll have a slightly weird encoding scheme: categories are ordered by their levels too. That is, in the numerical order will be all categories on the first level, THEN those on the second, so on and so forth.\n",
    "\n",
    "The first step to achieving this would be to separate each level into their own column. Then, for categorical encoding to work, the columns themselves must be in pandas' `category` datatype, instead of the default `object` type for non-numerical columns.\n",
    "\n",
    "From this, we can easily generate one-hot encodings for each level, which when concatenated gives the n-hot encoding for all categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 850,
     "status": "ok",
     "timestamp": 1635307918852,
     "user": {
      "displayName": "Khiêm Huỳnh Thiện",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKsMYrfMtAzOaIk6G51GUQwDS0P6dMkGSwN9B_aA=s64",
      "userId": "01825679077701569078"
     },
     "user_tz": -420
    },
    "id": "j8YYWA6NNUr4",
    "outputId": "44e296f1-93a6-448a-ea78-b124b62b3d2e"
   },
   "outputs": [],
   "source": [
    "def preprocess_classes(data, original_name, depth, verbose=False):\n",
    "    \"\"\"\n",
    "    Build a list of unique class names for each level and create bidirectional mappings.\n",
    "    \"\"\"\n",
    "    cls2idx = []\n",
    "    idx2cls = []\n",
    "    for i in range(depth): \n",
    "        category_li = data[original_name].apply(\n",
    "            lambda lst: lst[i]\n",
    "        ).astype('category')\n",
    "        if verbose:\n",
    "            print(category_li.cat.classes)\n",
    "        cls2idx.append(dict([\n",
    "            (category, index) \n",
    "            for (index, category) \n",
    "            in enumerate(category_li.cat.categories)\n",
    "        ]))\n",
    "        idx2cls.append(list(category_li.cat.categories))\n",
    "    return cls2idx, idx2cls\n",
    "\n",
    "cls2idx, idx2cls = preprocess_classes(data, CLASS_COL_NAME, DEPTH)    \n",
    "print(cls2idx)\n",
    "print('\\n')\n",
    "print(idx2cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1635307948128,
     "user": {
      "displayName": "Khiêm Huỳnh Thiện",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKsMYrfMtAzOaIk6G51GUQwDS0P6dMkGSwN9B_aA=s64",
      "userId": "01825679077701569078"
     },
     "user_tz": -420
    },
    "id": "AasvSxmA0HIu",
    "outputId": "003f50f4-43fa-4b55-d513-c8913ffebd24"
   },
   "outputs": [],
   "source": [
    "def class_to_index(data, original_name, cls2idx, depth):\n",
    "    data['codes'] = data[original_name].apply(\n",
    "        lambda lst: [\n",
    "            cls2idx[i][cat] \n",
    "            for (i, cat) \n",
    "            in enumerate(lst[:depth])\n",
    "        ]\n",
    "    ).astype('object')\n",
    "\n",
    "class_to_index(data, CLASS_COL_NAME, cls2idx, DEPTH)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyrIcY8O3Pjl"
   },
   "source": [
    "We can try recovering category names from this encoding to ensure consistency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1635308025470,
     "user": {
      "displayName": "Khiêm Huỳnh Thiện",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKsMYrfMtAzOaIk6G51GUQwDS0P6dMkGSwN9B_aA=s64",
      "userId": "01825679077701569078"
     },
     "user_tz": -420
    },
    "id": "_-Pvksp33XHj",
    "outputId": "b6f190b6-d420-4ba1-9688-9e1e2d4e1dc9"
   },
   "outputs": [],
   "source": [
    "def retrieve_classes(codes, idx2cls):\n",
    "    return [ idx2cls[i][code] for (i, code) in enumerate(codes) ]\n",
    "\n",
    "print('Original:', data.iloc[3]['category'][0:DEPTH])\n",
    "\n",
    "print('Retrieved:', retrieve_classes(data['codes'].iloc[3], idx2cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Smg58QeJ6jK-"
   },
   "source": [
    "# Hierarchy generation\n",
    "In this model, the hierarchical error penalty is simply $L_H = \\lambda \\times max(0,Y_{child} - Y_{parent})$. As such, we need to keep track of each node's parent and vectorise the calculation.\n",
    "\n",
    "For now I'll be implementing this as simple arrays of category codes (in the global categorical space). We can then use these arrays of codes as vectorised indices to pull out $Y_{parent}$s and have our loss function somewhat vectorised too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDMbz01-BORD"
   },
   "outputs": [],
   "source": [
    "# TODO: Bring the above code into this thing's constructor entirely.\n",
    "from functools import reduce\n",
    "class PerLevelHierarchy:\n",
    "    # level_sizes is a list of (distinct) class counts per hierarchical level.\n",
    "    #   Its length dictates the maximum hierarchy construction depth.\n",
    "    #   (that is, our above code)\n",
    "    # classes is the list of distinct classes, in the order we have assembled.\n",
    "    def __init__(self, data, cls2idx):\n",
    "        self.levels = [ len(d.keys()) for d in cls2idx ] # TODO: Rename to level_sizes\n",
    "        self.classes = reduce(lambda acc, elem: acc + elem, [ list(d.keys()) for d in cls2idx ], [])\n",
    "        # Where each level starts in a global n-hot category vector\n",
    "        # Its last element is coincidentally the length, which also allows us\n",
    "        # to simplify the slicing code by blindly doing [offset[i] : offset[i+1]]\n",
    "        self.level_offsets = reduce(lambda acc, elem: acc + [acc[len(acc) - 1] + elem], self.levels, [0])\n",
    "        # Use -1 to indicate 'undiscovered'\n",
    "        parent_of = [[-1] * level_size for level_size in self.levels] \n",
    "        for lst in data['codes']:\n",
    "            # First-level classes' parent is root, but here we set them to themselves.\n",
    "            # This effectively zeroes out the hierarchical loss for this level.\n",
    "            parent_of[0][lst[0]] = lst[0]\n",
    "            for i in range(1, len(self.levels)):\n",
    "                child_idx = lst[i]\n",
    "                parent_idx = lst[i-1]\n",
    "                if parent_of[i][child_idx] == -1:\n",
    "                    parent_of[i][child_idx] = parent_idx\n",
    "            self.parent_of = []\n",
    "            for parent_lst in parent_of:\n",
    "                self.parent_of.append(torch.LongTensor(parent_lst).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1635308149130,
     "user": {
      "displayName": "Khiêm Huỳnh Thiện",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKsMYrfMtAzOaIk6G51GUQwDS0P6dMkGSwN9B_aA=s64",
      "userId": "01825679077701569078"
     },
     "user_tz": -420
    },
    "id": "zSI6Ket7Gj8v",
    "outputId": "58a86d5f-6ac5-40e8-dc63-024ebb6f64a0"
   },
   "outputs": [],
   "source": [
    "hierarchy = PerLevelHierarchy(data, cls2idx)\n",
    "hierarchy.parent_of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6K2Cm1Bq7Kl"
   },
   "source": [
    "# Data and model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRp5rd9W3rXm"
   },
   "source": [
    "## Installing DistilBERT\n",
    "Alternative to full-fat BERT, roughly matching its performance while being faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jx5V1QzS3wRX"
   },
   "outputs": [],
   "source": [
    "if not INSTALL_DISTILBERT:\n",
    "    os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "else:\n",
    "    !pip install transformers\n",
    "    \n",
    "import transformers as ppb\n",
    "tokenizer = ppb.DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "base_encoder = ppb.DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "base_encoder_state = base_encoder.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vr1iQObJQOPB"
   },
   "source": [
    "## Define our dataset adapter class\n",
    "This wraps around our data and provides a PyTorch-compatible interface.\n",
    "\n",
    "One point of interest is how we do not explicitly store per-level one-hot encodings. Here we simply store one global copy (n-hot-encoded) then slice from it when requested, saving a bit of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8cdh6oiQTsq"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, df, hierarchy, tokenizer, max_len, text_col_name = TEXT_COL_NAME):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.text = df[text_col_name]\n",
    "    # Level sizes\n",
    "    self.levels = hierarchy.levels\n",
    "    self.labels = df.codes\n",
    "    self.level_offsets = hierarchy.level_offsets\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.text)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    text = str(self.text.iloc[index])\n",
    "    text = \" \".join(text.split())\n",
    "    inputs = self.tokenizer(\n",
    "      text,\n",
    "      None, # No text_pair\n",
    "      add_special_tokens=True, # CLS, SEP\n",
    "      max_length=self.max_len, # For us it's a hyperparam. See next cells.\n",
    "      padding='max_length',\n",
    "      truncation=True\n",
    "      # BERT tokenisers return attention masks by default\n",
    "    )\n",
    "\n",
    "    labels = torch.tensor(self.labels.loc[index], dtype=torch.long)\n",
    "\n",
    "    result = {\n",
    "      'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "      'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "      'labels': labels,\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZywhH4tS2Q0"
   },
   "source": [
    "Regarding that `max_len` hyperparameter, let's see the distribution of title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 1200,
     "status": "ok",
     "timestamp": 1635308166461,
     "user": {
      "displayName": "Khiêm Huỳnh Thiện",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKsMYrfMtAzOaIk6G51GUQwDS0P6dMkGSwN9B_aA=s64",
      "userId": "01825679077701569078"
     },
     "user_tz": -420
    },
    "id": "LN-EM4dWS02O",
    "outputId": "c20c5dca-744d-4e14-bc99-679e8e68cb50",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['title_len'] = data['title'].apply(lambda s: len(s.split()))\n",
    "data['title_len'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZ33DAURTg3I"
   },
   "source": [
    "We prefer `max_len` to be a power of two that covers most of the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 337,
     "status": "ok",
     "timestamp": 1635308388604,
     "user": {
      "displayName": "Khiêm Huỳnh Thiện",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKsMYrfMtAzOaIk6G51GUQwDS0P6dMkGSwN9B_aA=s64",
      "userId": "01825679077701569078"
     },
     "user_tz": -420
    },
    "id": "wpFTDAgDq_oV",
    "outputId": "ca0d543e-98ae-4126-c716-a1a3c056dfab"
   },
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 64\n",
    "LINEAR_DROPOUT_RATE = 0.3\n",
    "TRAIN_MINIBATCH_SIZE = 16\n",
    "VAL_TEST_MINIBATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "ENCODER_LEARNING_RATE = 3e-05\n",
    "CLASSIFIER_LEARNING_RATE = 3e-03\n",
    "TRAIN_SET_RATIO = 0.8\n",
    "VAL_SET_RATIO = 0.1\n",
    "# The rest is test set\n",
    "# Don't change this if you want a consistent sampling for easier comparisons\n",
    "RANDOM_SEED = 123\n",
    "\n",
    "FULL_SET = False\n",
    "PARTIAL_SET_FRAC = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vRe05MEHZ9g"
   },
   "source": [
    "CV-split our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1211,
     "status": "ok",
     "timestamp": 1635308170890,
     "user": {
      "displayName": "Khiêm Huỳnh Thiện",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKsMYrfMtAzOaIk6G51GUQwDS0P6dMkGSwN9B_aA=s64",
      "userId": "01825679077701569078"
     },
     "user_tz": -420
    },
    "id": "8973_cElHYg2",
    "outputId": "db7e22e7-b868-43ec-a114-4294c462cda3"
   },
   "outputs": [],
   "source": [
    "small_data = None\n",
    "if not FULL_SET:\n",
    "    small_data = data.sample(frac = PARTIAL_SET_FRAC, random_state=RANDOM_SEED)\n",
    "\n",
    "train_set = None\n",
    "test_set = None\n",
    "\n",
    "COLUMNS = [TEXT_COL_NAME, 'codes']\n",
    "\n",
    "filtered = None\n",
    "if FULL_SET:\n",
    "    filtered = data[COLUMNS]\n",
    "else:\n",
    "    filtered = small_data[COLUMNS]\n",
    "\n",
    "train_set = filtered.sample(frac = TRAIN_SET_RATIO, random_state=RANDOM_SEED)\n",
    "val_test_set = filtered.drop(train_set.index)\n",
    "\n",
    "val_set = val_test_set.sample(frac = VAL_SET_RATIO / (1-TRAIN_SET_RATIO), random_state=RANDOM_SEED)\n",
    "test_set = val_test_set.drop(val_set.index)\n",
    "\n",
    "train_set = train_set.reset_index(drop=True)\n",
    "val_set = val_set.reset_index(drop=True)\n",
    "test_set = test_set.reset_index(drop=True)\n",
    "\n",
    "print(train_set.shape)\n",
    "print(val_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PXLfuJcV2aK"
   },
   "source": [
    "We can now wrap them in our Datasets, and then into PyTorch's DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_b1j_gNdV9Ux"
   },
   "outputs": [],
   "source": [
    "train_set_wrapped = CustomDataset(train_set, hierarchy, tokenizer, MAX_LEN)\n",
    "val_set_wrapped = CustomDataset(val_set, hierarchy, tokenizer, MAX_LEN)\n",
    "test_set_wrapped = CustomDataset(test_set, hierarchy, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set_wrapped, batch_size=TRAIN_MINIBATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_set_wrapped, batch_size=VAL_TEST_MINIBATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set_wrapped, batch_size=VAL_TEST_MINIBATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGLkaGimW285"
   },
   "source": [
    "## Prepare the model itself\n",
    "Here we use DistilBERT as the encoding layers, followed by our implementation of HMCN-F."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4yHw2BlRhLM"
   },
   "source": [
    "### DB-FBHCN\n",
    "This is our hybrid between HMCN-F and our simpler BERT-linear model.\n",
    "\n",
    "The idea is simple: one layer per hierarchical level. First layer takes in 768 BERT features (we might want to pool it later) and output $h_1$ values corresponding to scores for all categories at the first level. The second layer takes in this prediction, plus 768 original BERT features, and output $h_2$ values corresponding to scores for all categories on the second level, and so on.\n",
    "\n",
    "The final hierarchical prediction is simply the concatenation of predictions from each level, from first to last. Each would have already been through a sigmoid function before being concatenated, but that would be done outside of the model for performance reasons.\n",
    "\n",
    "Right now the global loss calculation is simply the sum of losses of each level. For this reason we make the model output each level separately. That way we can also easily argmax on each level to get the category names.\n",
    "\n",
    "We'll keep the hierarchical loss, as it is important in keeping the levels in sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGFxM_q2RpkC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class DBFBHCN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        feature_size, \n",
    "        dropout_rate, # after every linear layer\n",
    "        hierarchy,\n",
    "        hidden_nonlinear='relu'\n",
    "    ):\n",
    "        super(DBFBHCN, self).__init__()\n",
    "\n",
    "        # Back up some parameters for use in forward()\n",
    "        self.depth = len(hierarchy.levels)\n",
    "\n",
    "        # First layer only takes in BERT encodings\n",
    "        self.fc_layers = torch.nn.ModuleList([ \n",
    "            torch.nn.Linear(feature_size, hierarchy.levels[0])\n",
    "        ])\n",
    "        torch.nn.init.xavier_uniform_(self.fc_layers[0].weight)\n",
    "        self.norms = torch.nn.ModuleList([])\n",
    "        for i in range(1, self.depth):\n",
    "            self.fc_layers.extend([ \n",
    "                torch.nn.Linear(feature_size + hierarchy.levels[i-1], hierarchy.levels[i]) \n",
    "            ])\n",
    "            torch.nn.init.xavier_uniform_(self.fc_layers[i].weight)\n",
    "            self.norms.extend([torch.nn.LayerNorm(hierarchy.levels[i-1], elementwise_affine=False)])\n",
    "        # Activation functions\n",
    "        self.hidden_nonlinear = torch.nn.ReLU() if hidden_nonlinear == 'relu' else torch.nn.Tanh()\n",
    "        self.output_nonlinear = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # We have |D| of these\n",
    "        local_outputs = []\n",
    "        output_l1 = self.fc_layers[0](self.dropout(x))\n",
    "        local_outputs.append(self.output_nonlinear(output_l1))\n",
    "\n",
    "        prev_output = self.hidden_nonlinear(output_l1)\n",
    "        for i in range(1, self.depth):\n",
    "            output_li = self.fc_layers[i](torch.cat([self.dropout(self.norms[i-1](prev_output)), x], dim=1))\n",
    "            local_outputs.append(self.output_nonlinear(output_li))\n",
    "            prev_output = self.hidden_nonlinear(output_li)\n",
    "\n",
    "        return local_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_btMadIfe7A"
   },
   "source": [
    "## Checkpoints\n",
    "PyTorch allows us to save the best-performing model automatically so we can restart from that instead of the beginning. No reason not to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mIDVu-5Ffd6v"
   },
   "outputs": [],
   "source": [
    "import shutil, sys\n",
    "def load_checkpoint(checkpoint_fpath, model):\n",
    "    \"\"\"\n",
    "    checkpoint_fpath: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into       \n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "    \n",
    "    encoder, classifier = model\n",
    "    \n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    classifier.load_state_dict(checkpoint['classifier_state_dict'])\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "\n",
    "    # return model, optimizer, epoch value, min validation loss \n",
    "    return (encoder, classifier)\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint_path, best_model_path):\n",
    "    \"\"\"\n",
    "    state: checkpoint we want to save\n",
    "    is_best: is this the best checkpoint; min validation loss\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    best_model_path: path to save best model\n",
    "    \"\"\"\n",
    "\n",
    "    f_path = checkpoint_path\n",
    "    # save checkpoint data to the path given, checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    # if it is a best model (min validation lost)\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        # copy that checkpoint file to best path given, best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFA8W-4CvOH9"
   },
   "source": [
    "# Training time\n",
    "## Metrics\n",
    "We define hierarchical accuracy as simply the averaged accuracy over each level. Same for precision.\n",
    "This is a big TODO item: get something more representative for metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r287BexWv69M"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def get_metrics(local_outputs, targets, log_metrics = False, get_auprc = False):\n",
    "    leaf_size = local_outputs[-1].shape[1]\n",
    "    def generate_one_hot(idx):\n",
    "        b = np.zeros(leaf_size, dtype=bool)\n",
    "        b[idx] = 1\n",
    "        return b\n",
    "\n",
    "    level_codes = [ \n",
    "        np.argmax(local_outputs[level], axis=1) \n",
    "        for level in range(len(local_outputs)) \n",
    "    ]\n",
    "    \n",
    "    accuracies = [ metrics.accuracy_score(level_codes[level], targets[:, level]) for level in range(len(hierarchy.levels)) ]\n",
    "    precisions = [ metrics.precision_score(level_codes[level], targets[:, level], average='weighted', zero_division=0) for level in range(len(local_outputs)) ]\n",
    "    \n",
    "    global_accuracy = sum(accuracies)/len(accuracies)\n",
    "    global_precision = sum(precisions)/len(precisions)\n",
    "    \n",
    "    if log_metrics:\n",
    "        logging.info('Leaf level:')\n",
    "        logging.info(\"Accuracy: {}\".format(accuracies[-1]))\n",
    "        # Model Precision: what percentage of positive tuples are labeled as such?\n",
    "        logging.info(\"Precision: {}\".format(precisions[-1]))\n",
    "\n",
    "        logging.info('Global level:')\n",
    "        logging.info(\"Accuracy: {}\".format(global_accuracy))\n",
    "        # Model Precision: what percentage of positive tuples are labeled as such?\n",
    "        logging.info(\"Precision: {}\".format(global_precision))\n",
    "    \n",
    "    if get_auprc:\n",
    "        binarised_targets = np.array([generate_one_hot(lst[-1]) for lst in targets])\n",
    "\n",
    "        # Rectified leaf AU(PRC) due to an sklearn bug.\n",
    "        # We add one artificial example that belongs to all classes at once and a corresponding prediction\n",
    "        # full of true positives. This way each class has at least one true positive, even if the test set\n",
    "        # does not contain enough examples to cover all classes.\n",
    "        rectified_outputs = np.concatenate([local_outputs[-1], np.ones((1, leaf_size))], axis=0)\n",
    "        rectified_targets = np.concatenate([binarised_targets, np.ones((1, leaf_size), dtype=bool)], axis=0)\n",
    "        auprc = metrics.average_precision_score(rectified_targets, rectified_outputs)\n",
    "        if log_metrics:\n",
    "            logging.info('Rectified leaf-level AU(PRC) score: {}'.format(auprc))\n",
    "    \n",
    "        return np.array([accuracies[-1], precisions[-1], global_accuracy, global_precision, auprc])\n",
    "    return np.array([accuracies[-1], precisions[-1], global_accuracy, global_precision])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgdVf_dMv69N"
   },
   "source": [
    "## Training script\n",
    "\n",
    "Now we define the loss function that we will use to fine-tune our model (DistilBERT included).\n",
    "\n",
    "For now we'll stick with one of the provided loss functions instead of building anything radical. As we are performing multiclass classification here, we should use Cross Entropy Loss (the normal one for multiclass, not BCE for binary). The way we use them however, is a bit different. We'll have two types of loss values:\n",
    "- A leaf-level loss value, that is, $L_L = NLL(y_{leaf}, \\bar{y}_{leaf})$, and\n",
    "- A hierarchical loss value $L_H = NLL(y_{parent}, \\bar{y}_{parent})$ for every level except leaf, summed up. That is, we will enforce that each level must predict the parent of whatever the next level predicted, even if they are wrong, creating a bottom-up hierarchy tracing strategy. This will be kept at a lower significance than the leaf-level loss due to its tendency to explode and knocking our gradient descent out of a good path.\n",
    "\n",
    "The final loss is then defined as $L = \\lambda_L L_L + \\lambda_H L_H$ where $\\lambda_H$ is likely to be smaller than $\\lambda_L$.\n",
    "\n",
    "We also construct our optimiser here, which is Adam with different learning rates for DistilBERT and our model.\n",
    "\n",
    "All of these are put inside a `train_model` function for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxqdEcgMvPoV"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_model(checkpoint_path, best_model_path, start_epochs, n_epochs, val_loss_min_input, \n",
    "                training_loader, val_loader, model, hierarchy, lambda_L, lambda_H, gamma_L, verbose=False\n",
    "                ):\n",
    "    encoder, classifier = model\n",
    "    \n",
    "    depth = len(hierarchy.levels)\n",
    "    \n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    criterion_h = torch.nn.NLLLoss(reduction='none')\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [\n",
    "            {'params': encoder.parameters(), 'lr': ENCODER_LEARNING_RATE},\n",
    "            {'params': classifier.parameters(), 'lr': CLASSIFIER_LEARNING_RATE}\n",
    "        ], \n",
    "    )\n",
    "    \n",
    "    deviations = np.linspace(-gamma_L, gamma_L, depth)\n",
    "    loss_L_weights = [1] * depth\n",
    "    loss_L_weights -= deviations\n",
    "    print('Using level weights', loss_L_weights, 'for local loss.')\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [\n",
    "            {'params': encoder.parameters(), 'lr': ENCODER_LEARNING_RATE,},\n",
    "            {'params': classifier.parameters(), 'lr': CLASSIFIER_LEARNING_RATE}\n",
    "        ], \n",
    "    )\n",
    "\n",
    "    # Keep min validation loss so we can separately back up our best-yet model\n",
    "    val_loss_min = val_loss_min_input\n",
    "    \n",
    "    # Store validation metrics after each epoch\n",
    "    val_metrics = np.empty((4, 0), dtype=float)\n",
    "    \n",
    "    for epoch in range(start_epochs, n_epochs+1):\n",
    "        train_loss = 0\n",
    "        # Put model into training mode. Note that this call DOES NOT train it yet.\n",
    "        encoder.train()\n",
    "        classifier.train()\n",
    "        print('Epoch {}: Training'.format(epoch))\n",
    "        for batch_idx, data in enumerate(tqdm(training_loader)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['labels']#.to(device, dtype = torch.long)\n",
    "\n",
    "            features = encoder(ids, mask)[0][:,0,:]\n",
    "            local_outputs = classifier(features)\n",
    "\n",
    "            # We have two loss functions: (l)ocal (per-level), and (h)ierarchical.\n",
    "            loss_l = lambda_L * sum([ criterion(\n",
    "                local_outputs[level].cpu(),\n",
    "                targets[:, level]\n",
    "            ) * loss_L_weights[level] for level in range(depth) ])\n",
    "            \n",
    "            # Hierarchically penalise less (or don't at all) if the prediction itself is wrong at the child level.\n",
    "            loss_h_levels = []\n",
    "            for level in range(depth-1):\n",
    "                target_child_indices = torch.unsqueeze(targets[:, level + 1], 1).to(device)\n",
    "                transformed = local_outputs[level + 1] * -1\n",
    "                transformed -= transformed.min(1, keepdim=True)[0]\n",
    "                transformed /= transformed.max(1, keepdim=True)[0]\n",
    "                loss_factors = 1 - torch.squeeze(transformed.gather(1, target_child_indices), 1)\n",
    "                loss_h_levels.append(\n",
    "                    torch.mean(criterion_h(\n",
    "                        local_outputs[level], \n",
    "                        torch.index_select(\n",
    "                            hierarchy.parent_of[level + 1],\n",
    "                            0, \n",
    "                            torch.argmax(local_outputs[level + 1], dim=1)\n",
    "                        )\n",
    "                    ) * loss_factors)\n",
    "                )\n",
    "            loss_h = lambda_H * sum(loss_h_levels)\n",
    "            loss = loss_l + loss_h\n",
    "\n",
    "            # PyTorch defaults to accumulating gradients, but we don't need that here\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss = train_loss + (loss.item() - train_loss) / (batch_idx + 1)\n",
    "\n",
    "        print('Epoch {}: Validating'.format(epoch))\n",
    "\n",
    "\n",
    "        # Switch to evaluation (prediction) mode. Again, this doesn't evaluate anything.\n",
    "        encoder.eval()\n",
    "        classifier.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        val_targets = np.empty((0, len(hierarchy.levels)), dtype=bool)\n",
    "        val_outputs = [np.empty((0, hierarchy.levels[level]), dtype=float) for level in range(len(hierarchy.levels))]\n",
    "\n",
    "        # We're only testing here, so don't run the backward direction (no_grad).\n",
    "        with torch.no_grad():\n",
    "            total_loss_l = 0\n",
    "            total_loss_h = 0\n",
    "            for batch_idx, data in enumerate(tqdm(val_loader)):\n",
    "                ids = data['ids'].to(device, dtype = torch.long)\n",
    "                mask = data['mask'].to(device, dtype = torch.long)\n",
    "                targets = data['labels']#.to(device, dtype = torch.long)\n",
    "\n",
    "                features = encoder(ids, mask)[0][:,0,:]\n",
    "                local_outputs = classifier(features)\n",
    "\n",
    "                # We have two loss functions: (l)ocal (per-level), and (h)ierarchical.\n",
    "                loss_l = lambda_L * sum([ criterion(\n",
    "                    local_outputs[level].cpu(),\n",
    "                    targets[:, level]\n",
    "                ) * loss_L_weights[level] for level in range(depth) ])\n",
    "\n",
    "                # Hierarchically penalise less (or don't at all) if the prediction itself is wrong at the child level.\n",
    "                loss_h_levels = []\n",
    "                for level in range(depth-1):\n",
    "                    target_child_indices = torch.unsqueeze(targets[:, level + 1], 1).to(device)\n",
    "                    transformed = local_outputs[level + 1] * -1\n",
    "                    transformed -= transformed.min(1, keepdim=True)[0]\n",
    "                    transformed /= transformed.max(1, keepdim=True)[0]\n",
    "                    loss_factors = 1 - torch.squeeze(transformed.gather(1, target_child_indices), 1)\n",
    "                    loss_h_levels.append(\n",
    "                        torch.mean(criterion_h(\n",
    "                            local_outputs[level], \n",
    "                            torch.index_select(\n",
    "                                hierarchy.parent_of[level + 1],\n",
    "                                0, \n",
    "                                torch.argmax(local_outputs[level + 1], dim=1)\n",
    "                            )\n",
    "                        ) * loss_factors)\n",
    "                    )\n",
    "                loss_h = lambda_H * sum(loss_h_levels)\n",
    "                loss = loss_l + loss_h\n",
    "                \n",
    "                total_loss_l += loss_l\n",
    "                total_loss_h += loss_h\n",
    "\n",
    "                val_loss = val_loss + (loss.item() - val_loss) / (batch_idx + 1)\n",
    "\n",
    "                val_targets = np.concatenate([val_targets, targets.cpu().detach().numpy()])\n",
    "                \n",
    "                for i in range(len(val_outputs)):\n",
    "                    val_outputs[i] = np.concatenate([val_outputs[i], local_outputs[i].cpu().detach().numpy()])\n",
    "\n",
    "        val_metrics = np.concatenate([val_metrics, \n",
    "            np.expand_dims(\n",
    "                get_metrics(val_outputs, val_targets), axis=1\n",
    "            )],\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        train_loss = train_loss/len(training_loader)\n",
    "        val_loss = val_loss/len(val_loader)\n",
    "        \n",
    "        if verbose:\n",
    "            # Calculate average losses\n",
    "            print('Average minibatch local loss:', total_loss_l / len(val_loader))\n",
    "            print('Average minibatch hierarchical loss:', total_loss_h / len(val_loader))\n",
    "\n",
    "            # Print training/validation statistics\n",
    "            print('Avgerage training loss: {:.6f}\\nAverage validation loss: {:.6f}'.format( \n",
    "                train_loss,\n",
    "                val_loss\n",
    "            ))\n",
    "\n",
    "        # create checkpoint variable and add important data\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'val_loss_min': val_loss,\n",
    "            'encoder_state_dict': encoder.state_dict(),\n",
    "            'classifier_state_dict': classifier.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "        best_yet = False\n",
    "        if val_loss <= val_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(val_loss_min,val_loss))\n",
    "            # save checkpoint as best model\n",
    "            best_yet = True\n",
    "            val_loss_min = val_loss\n",
    "        save_checkpoint(checkpoint, best_yet, checkpoint_path, best_model_path)\n",
    "        print('Epoch {}: Done\\n'.format(epoch))\n",
    "    return (encoder, classifier), val_metrics\n",
    "\n",
    "# Alternative: just load from disk\n",
    "def run_model(model, loader, hierarchy):\n",
    "  encoder, classifier = model\n",
    "  # Switch to evaluation (prediction) mode. Again, this doesn't evaluate anything.\n",
    "  encoder.eval()\n",
    "  classifier.eval()\n",
    "\n",
    "  all_targets = np.empty((0, len(hierarchy.levels)), dtype=bool)\n",
    "  all_outputs = [np.empty((0, hierarchy.levels[level]), dtype=float) for level in range(len(hierarchy.levels))]\n",
    "\n",
    "  # We're only testing here, so don't run the backward direction (no_grad).\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(tqdm(loader)):\n",
    "      ids = data['ids'].to(device, dtype = torch.long)\n",
    "      mask = data['mask'].to(device, dtype = torch.long)\n",
    "      targets = data['labels'].to(device, dtype = torch.long)\n",
    "\n",
    "      features = encoder(ids, mask)[0][:,0,:]\n",
    "      local_outputs = classifier(features)\n",
    "\n",
    "      all_targets = np.concatenate([all_targets, targets.cpu().detach().numpy()])\n",
    "      for i in range(len(all_outputs)):\n",
    "        all_outputs[i] = np.concatenate([all_outputs[i], local_outputs[i].cpu().detach().numpy()])\n",
    "\n",
    "  return {\n",
    "      'targets': all_targets,\n",
    "      'outputs': all_outputs,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcqgkF06fnkf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_name = 'checkpoints-avg-' + DATASET_NAME\n",
    "!mkdir $folder_name\n",
    "checkpoint_idx_path = \"{}/last_idx\".format(folder_name)\n",
    "\n",
    "# Set up current checkpoint index\n",
    "CHECKPOINT_IDX = 0\n",
    "if os.path.exists(checkpoint_idx_path):\n",
    "    idx_file = open(checkpoint_idx_path, 'r')\n",
    "    CHECKPOINT_IDX = int(idx_file.read()) + 1\n",
    "    idx_file.close()\n",
    "idx_file = open(checkpoint_idx_path, 'w')\n",
    "idx_file.write(str(CHECKPOINT_IDX))\n",
    "idx_file.close()\n",
    "\n",
    "logging.basicConfig(filename=\"{}/{}.log\".format(folder_name, CHECKPOINT_IDX), level=logging.INFO)\n",
    "\n",
    "all_test_metrics = np.zeros((TRAIN_REPEATS, 5), dtype=float)\n",
    "\n",
    "trained_model = None\n",
    "if TRAIN_FROM_SCRATCH:\n",
    "    for i in range(TRAIN_REPEATS):\n",
    "        run_header = \"--- RUN {} ---\".format(i)\n",
    "        print(run_header)\n",
    "        logging.info(run_header)\n",
    "        # Reinitialise weights\n",
    "        encoder = base_encoder\n",
    "        encoder.load_state_dict(base_encoder_state)\n",
    "        encoder.to(device)\n",
    "        classifier = DBFBHCN(\n",
    "            768, # DistilBERT outputs 768 values.\n",
    "            dropout_rate = LINEAR_DROPOUT_RATE,\n",
    "            hierarchy=hierarchy,\n",
    "            hidden_nonlinear='relu',\n",
    "        )\n",
    "        classifier.to(device)\n",
    "        \n",
    "        # Train/validate\n",
    "        CHECKPOINT_PATH = './{}/{}_{}_current.pt'.format(folder_name, CHECKPOINT_IDX, i)\n",
    "        BEST_CHECKPOINT_PATH = './{}/{}_{}_best.pt'.format(folder_name, CHECKPOINT_IDX, i)\n",
    "        trained_model, val_metrics = train_model(\n",
    "            CHECKPOINT_PATH,\n",
    "            BEST_CHECKPOINT_PATH,\n",
    "            1, # Count from one\n",
    "            EPOCHS,\n",
    "            np.Inf, \n",
    "            train_loader, \n",
    "            val_loader, \n",
    "            (encoder, classifier), \n",
    "            hierarchy,\n",
    "            1.0, # lambda_L\n",
    "            0.7, # lambda_H\n",
    "            -0.25 # gamma_L\n",
    "        )\n",
    "        \n",
    "        # Test\n",
    "        test_result = run_model(trained_model, test_loader, hierarchy)\n",
    "        test_metrics = get_metrics(test_result['outputs'], test_result['targets'], True, True)\n",
    "        all_test_metrics[i, :] = test_metrics\n",
    "    averaged = np.average(all_test_metrics, axis = 0)\n",
    "    averaged_display = '--- Average of {} runs:\\nLeaf accuracy: {}\\nLeaf precision: {}\\nPath accuracy: {}\\nPath precision: {}\\nLeaf AU(PRC): {}'.format(\n",
    "        TRAIN_REPEATS, averaged[0], averaged[1], averaged[2], averaged[3], averaged[4])\n",
    "    print(averaged_display)\n",
    "    logging.info(averaged_display)\n",
    "else:\n",
    "    load_path = '{}/{}_{}_{}.pt'.format(folder_name, LOAD_ITERATION, TRAIN_REPEATS - 1, 'best' if LOAD_BEST else 'current')\n",
    "    trained_model, _ = load_checkpoint(load_path, (encoder, classifier))\n",
    "    # Test\n",
    "    test_result = run_model(trained_model, test_loader, hierarchy)\n",
    "    get_metrics(test_result['outputs'], test_result['targets'], True, True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DB-FBHCN",
   "provenance": [
    {
     "file_id": "1XH71XbNfkOyYkVwthbl_Mnt9YN0hwUv2",
     "timestamp": 1633613457791
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
