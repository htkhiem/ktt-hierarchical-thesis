{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8ssvaYmaBia"
   },
   "outputs": [],
   "source": [
    "### Dataset configuration\n",
    "# The parquet folder. It should be located inside datasets/.\n",
    "DATASET_NAME   = 'Walmart_30k.parquet'\n",
    "# The input text column\n",
    "TEXT_COL_NAME  = 'title'\n",
    "# Which column to use as labelled classes. It should be a column of lists of strings.\n",
    "CLASS_COL_NAME = 'category'\n",
    "# How many hierarchical levels to work on. Note that the dataset must also have at least this many levels for every example.\n",
    "DEPTH = 2\n",
    "\n",
    "### Checkpoint configuration\n",
    "# Whether to train from scratch or to load a checkpoint\n",
    "TRAIN_FROM_SCRATCH=True\n",
    "# If training from scratch, train this many times before averaging test set results. Train/val/test split is kept static.\n",
    "TRAIN_REPEATS = 2\n",
    "# Checkpoint iteration to load if not training from scratch\n",
    "LOAD_ITERATION=1\n",
    "# Last or best results from that iteration?\n",
    "LOAD_BEST=True\n",
    "\n",
    "### System configuration\n",
    "# Will try to use your NVIDIA GPU if one is available. Set to False to force CPU computation\n",
    "PREFER_GPU         = True\n",
    "# If you don't have the huggingface transformers library installed, flip this to True.\n",
    "# You only need to do this once. Once DistilBERT has been downloaded, it will be cached in your system's default user cache folder.\n",
    "# Once it is cached, please set this to False to avoid redownloads.\n",
    "INSTALL_DISTILBERT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9r_iHMmx75DM"
   },
   "source": [
    "# Import common libraries\n",
    "And also set up a few things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shaFdRkD74o1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set up GPU if available\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() and PREFER_GPU else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TO5DolJKZfNv",
    "outputId": "f788701e-7f29-4391-cb82-fd2d60c3acfa"
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTb90c5tRJsf"
   },
   "source": [
    "# Import data\n",
    "Here we'll finally be using the randomly-sampled 750k-row subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8e3fhFSRPPw",
    "outputId": "f62876fc-11c1-4a35-8534-aa4e27db8d07"
   },
   "outputs": [],
   "source": [
    "data = pd.read_parquet('../../datasets/{}'.format(DATASET_NAME))\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print (data.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96eLOv0hMvCv"
   },
   "source": [
    "# Categorical-encode the categories\n",
    "For this notebook, we'll only be classifying the leaf level. No hierarchy yet. Also, we'll be using a linear layer in PyTorch, which necessitates us to encode these categories manually.\n",
    "\n",
    "The reason we're not using one-hot encoding is because we're about to use cross-entropy loss for this notebook. Also see corresponding link in References for why we picked this.\n",
    "\n",
    "For categorical encoding to work, the column itself must be in pandas' `category` datatype, instead of the default `object` type for non-numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8YYWA6NNUr4",
    "outputId": "44e296f1-93a6-448a-ea78-b124b62b3d2e"
   },
   "outputs": [],
   "source": [
    "LEVEL = DEPTH-1\n",
    "leaf_categories = data['category'].apply(lambda lst: lst[min(LEVEL, len(lst) - 1)]).astype('category')\n",
    "data['targets'] = leaf_categories.cat.codes\n",
    "\n",
    "data['targets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb80Y48WZfN7"
   },
   "source": [
    "Keep a mapping between category code and category names (in strings) so we can get human-readable predictions later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AasvSxmA0HIu",
    "outputId": "003f50f4-43fa-4b55-d513-c8913ffebd24"
   },
   "outputs": [],
   "source": [
    "leaves = leaf_categories.cat.categories\n",
    "leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMYK-8pR0v6L"
   },
   "source": [
    "Also keep the number of leaves for later model construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJ8mFJ2R1EpD"
   },
   "outputs": [],
   "source": [
    "leaf_count = len(leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4sXPxkKzUK2"
   },
   "source": [
    "Now we can generate our custom-ordered, global binary encoding.\n",
    "\n",
    "$|Y_L^h|$ can easily be sliced from the global encoding by relying on the backed-up per-level class counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6K2Cm1Bq7Kl"
   },
   "source": [
    "# Data and model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRp5rd9W3rXm"
   },
   "source": [
    "## Installing DistilBERT\n",
    "Alternative to full-fat BERT, roughly matching its performance while being faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jx5V1QzS3wRX"
   },
   "outputs": [],
   "source": [
    "if not INSTALL_DISTILBERT:\n",
    "    os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "else:\n",
    "    !pip install transformers\n",
    "    \n",
    "import transformers as ppb\n",
    "tokenizer = ppb.DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "base_encoder = ppb.DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "base_encoder_state = base_encoder.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vr1iQObJQOPB"
   },
   "source": [
    "## Define our dataset adapter class\n",
    "This wraps around our data and provides a PyTorch-compatible interface.\n",
    "\n",
    "One point of interest is how we do not explicitly store per-level one-hot encodings. Here we simply store one global copy (n-hot-encoded) then slice from it when requested, saving a bit of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8cdh6oiQTsq"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, df, tokenizer, max_len):\n",
    "    self.tokenizer = tokenizer\n",
    "    # TODO: Make these customisable from the constructor interface\n",
    "    self.title = df['title']\n",
    "    self.labels = df['targets']\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.title)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    title = str(self.title.iloc[index])\n",
    "    title = \" \".join(title.split())\n",
    "    inputs = self.tokenizer(\n",
    "      title,\n",
    "      None, # No text_pair\n",
    "      add_special_tokens=True, # CLS, SEP\n",
    "      max_length=self.max_len, # For us it's a hyperparam. See next cells.\n",
    "      padding='max_length',\n",
    "      return_token_type_ids=True,\n",
    "      truncation=True\n",
    "      # BERT tokenisers return attention masks by default\n",
    "    )\n",
    "    return {\n",
    "      'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "      'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "      'token_type_ids': torch.tensor(inputs['token_type_ids'], dtype=torch.long),\n",
    "      'labels': torch.tensor(self.labels.iloc[index], dtype=torch.float)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZywhH4tS2Q0"
   },
   "source": [
    "Regarding that `max_len` hyperparameter, let's see the distribution of title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "LN-EM4dWS02O",
    "outputId": "c20c5dca-744d-4e14-bc99-679e8e68cb50"
   },
   "outputs": [],
   "source": [
    "data['title_len'] = data['title'].apply(lambda s: len(s.split()))\n",
    "data['title_len'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZ33DAURTg3I"
   },
   "source": [
    "We prefer `max_len` to be a power of two that covers most of the titles. Here it seems 64 will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpFTDAgDq_oV"
   },
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 64\n",
    "LINEAR_DROPOUT_RATE = 0.25\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-05\n",
    "TRAIN_MINIBATCH_SIZE = 32\n",
    "VAL_TEST_MINIBATCH_SIZE = 64\n",
    "TRAIN_SET_RATIO = 0.8\n",
    "VAL_SET_RATIO = 0.1\n",
    "# The rest is test set\n",
    "RANDOM_SEED = 123\n",
    "# Flip to False for faster hyperparameter tuning. If False, only 5% of the full dataset is used. \n",
    "FULL_SET = True\n",
    "PARTIAL_SET_FRAC = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vRe05MEHZ9g"
   },
   "source": [
    "CV-split our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8973_cElHYg2",
    "outputId": "db7e22e7-b868-43ec-a114-4294c462cda3"
   },
   "outputs": [],
   "source": [
    "small_data = None\n",
    "if not FULL_SET:\n",
    "    small_data = data.sample(frac = PARTIAL_SET_FRAC, random_state=RANDOM_SEED)\n",
    "\n",
    "train_set = None\n",
    "test_set = None\n",
    "\n",
    "COLUMNS = [TEXT_COL_NAME, 'targets']\n",
    "\n",
    "filtered = None\n",
    "if FULL_SET:\n",
    "    filtered = data[COLUMNS]\n",
    "else:\n",
    "    filtered = small_data[COLUMNS]\n",
    "\n",
    "train_set = filtered.sample(frac = TRAIN_SET_RATIO, random_state=RANDOM_SEED)\n",
    "val_test_set = filtered.drop(train_set.index)\n",
    "\n",
    "val_set = val_test_set.sample(frac = VAL_SET_RATIO / (1-TRAIN_SET_RATIO), random_state=RANDOM_SEED)\n",
    "test_set = val_test_set.drop(val_set.index)\n",
    "\n",
    "train_set = train_set.reset_index(drop=True)\n",
    "val_set = val_set.reset_index(drop=True)\n",
    "test_set = test_set.reset_index(drop=True)\n",
    "\n",
    "print(train_set.shape)\n",
    "print(val_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PXLfuJcV2aK"
   },
   "source": [
    "We can now wrap them in our Datasets, and then into PyTorch's DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_b1j_gNdV9Ux"
   },
   "outputs": [],
   "source": [
    "train_set = CustomDataset(train_set, tokenizer, MAX_LEN)\n",
    "val_set = CustomDataset(val_set, tokenizer, MAX_LEN)\n",
    "test_set = CustomDataset(test_set, tokenizer, MAX_LEN)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=TRAIN_MINIBATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=VAL_TEST_MINIBATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=VAL_TEST_MINIBATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGLkaGimW285"
   },
   "source": [
    "## Prepare the model itself\n",
    "Here we use DistilBERT as the encoding layers, followed by a dropout layer and a linear layer for leaf category classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGFxM_q2RpkC",
    "outputId": "ec88f454-15c9-46ff-b6f7-376fc9a0473d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from functools import reduce\n",
    "\n",
    "class DistilBERTClass(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, output_count):\n",
    "    super(DistilBERTClass, self).__init__()\n",
    "    encoder = base_encoder\n",
    "    encoder.load_state_dict(base_encoder_state)\n",
    "    self.l1 = encoder\n",
    "    self.l2 = torch.nn.Dropout(LINEAR_DROPOUT_RATE)\n",
    "    self.l3 = torch.nn.Linear(768, output_count) # DistilBERT outputs 768 values.\n",
    "    self.output_size = output_count\n",
    "\n",
    "  def forward(self, ids, mask):\n",
    "    output_1 = self.l1(ids, attention_mask = mask)[0][:,0,:]\n",
    "    output_2 = self.l2(output_1)\n",
    "    output = self.l3(output_2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfHPZdswaBY8"
   },
   "source": [
    "Now we define the loss function that we will use to fine-tune our model (DistilBERT included).\n",
    "\n",
    "For now we'll stick with one of the provided loss functions instead of building anything radical. As we are performing multiclass classification here, we should use Cross Entropy Loss (the normal one for multiclass, not BCE for binary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXZXWUITaBF3"
   },
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_btMadIfe7A"
   },
   "source": [
    "## Checkpoints\n",
    "PyTorch allows us to save the best-performing model automatically so we can restart from that instead of the beginning. No reason not to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mIDVu-5Ffd6v"
   },
   "outputs": [],
   "source": [
    "import shutil, sys\n",
    "def load_checkpoint(checkpoint_fpath, model, optimizer):\n",
    "    \"\"\"\n",
    "    checkpoint_fpath: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into       \n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "\n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    # initialize optimizer from checkpoint to optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
    "    valid_loss_min = checkpoint['valid_loss_min']\n",
    "\n",
    "    # return model, optimizer, epoch value, min validation loss \n",
    "    return model, optimizer, checkpoint['epoch'], valid_loss_min\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint_path, best_model_path):\n",
    "    \"\"\"\n",
    "    state: checkpoint we want to save\n",
    "    is_best: is this the best checkpoint; min validation loss\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    best_model_path: path to save best model\n",
    "    \"\"\"\n",
    "\n",
    "    f_path = checkpoint_path\n",
    "    # save checkpoint data to the path given, checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    # if it is a best model (min validation lost)\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        # copy that checkpoint file to best path given, best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFA8W-4CvOH9"
   },
   "source": [
    "# Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCiOYWU3ZfOS"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def get_metrics(output, targets, log_metrics = False, get_auprc = False):\n",
    "    def generate_one_hot(idx):\n",
    "      b = np.zeros(leaf_count, dtype=bool)\n",
    "      b[idx] = 1\n",
    "      return b\n",
    "\n",
    "    # Get predicted codes\n",
    "    predicted_leaf_codes = np.argmax(output, axis=1)\n",
    "\n",
    "    acc = metrics.accuracy_score(predicted_leaf_codes, targets)\n",
    "    pre = metrics.precision_score(predicted_leaf_codes, targets, average='weighted', zero_division=0)\n",
    "\n",
    "    if log_metrics:\n",
    "      logging.info('Leaf level metrics:')\n",
    "      logging.info(\"Accuracy: {}\".format(acc))\n",
    "      # Model Precision: what percentage of positive tuples are labeled as such?\n",
    "      logging.info(\"Precision: {}\".format(pre))\n",
    "\n",
    "    if get_auprc:\n",
    "      # Rectified leaf AU(PRC) due to an sklearn bug.\n",
    "      # We add one artificial example that belongs to all classes at once and a corresponding prediction\n",
    "      # full of true positives. This way each class has at least one true positive, even if the test set\n",
    "      # does not contain enough examples to cover all classes.\n",
    "      binarised_targets = np.array([generate_one_hot(code) for code in targets])\n",
    "      rectified_outputs = np.concatenate([output, np.ones((1, leaf_count))], axis=0)\n",
    "      rectified_targets = np.concatenate([binarised_targets, np.ones((1, leaf_count), dtype=bool)], axis=0)\n",
    "      auprc = metrics.average_precision_score(rectified_targets, rectified_outputs)\n",
    "      if log_metrics:\n",
    "        logging.info('Rectified leaf-level AU(PRC) score: {}'.format(auprc))\n",
    "      return np.array([acc, pre, auprc])\n",
    "    return np.array([acc, pre])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxqdEcgMvPoV"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_model(start_epochs, n_epochs, val_loss_min_input, \n",
    "                training_loader, val_loader, model,\n",
    "                checkpoint_path, best_model_path, verbose=False):\n",
    "\n",
    "  # Store validation metrics after each epoch\n",
    "  val_metrics = np.empty((2, 0), dtype=float)\n",
    "\n",
    "  optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "  # Keep min validation (test set) loss so we can separately back up our best-yet model\n",
    "  val_loss_min = val_loss_min_input\n",
    "  for epoch in range(start_epochs, n_epochs+1):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    # Put model into training mode. Note that this call DOES NOT train it yet.\n",
    "    model.train()\n",
    "    print('Epoch {}: Training'.format(epoch))\n",
    "    for batch_idx, data in enumerate(tqdm(training_loader)):\n",
    "      ids = data['ids'].to(device, dtype = torch.long)\n",
    "      mask = data['mask'].to(device, dtype = torch.long)\n",
    "      targets = data['labels'].to(device, dtype = torch.long)\n",
    "\n",
    "      outputs = model(ids, mask)\n",
    "\n",
    "      loss = loss_fn(outputs, targets)\n",
    "\n",
    "      # PyTorch defaults to accumulating gradients, but we don't need that here\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss = train_loss + (loss.item() - train_loss) / (batch_idx + 1)\n",
    "\n",
    "    print('Epoch {}: Testing'.format(epoch))\n",
    "    \n",
    "    \n",
    "    # Switch to evaluation (prediction) mode. Again, this doesn't evaluate anything.\n",
    "    model.eval()\n",
    "\n",
    "    val_targets = np.array([], dtype=float)\n",
    "    val_outputs = np.empty((0, model.output_size), dtype=float)\n",
    "\n",
    "    # We're only testing here, so don't run the backward direction (no_grad).\n",
    "    with torch.no_grad():\n",
    "      total_loss = 0\n",
    "      batch_count = 0\n",
    "      for batch_idx, data in enumerate(tqdm(val_loader)):\n",
    "        batch_count += 1\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['labels'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask)\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "        val_loss = val_loss + (loss.item() - val_loss) / (batch_idx + 1)\n",
    "        \n",
    "        val_targets = np.concatenate([val_targets, targets.cpu().detach().numpy()])\n",
    "        val_outputs = np.concatenate([val_outputs, outputs.cpu().detach().numpy()])\n",
    "\n",
    "      # calculate average losses\n",
    "      #print('before cal avg train loss', train_loss)\n",
    "      if verbose:\n",
    "        print('Average minibatch loss:', total_loss / batch_count)\n",
    "    \n",
    "      val_metrics = np.concatenate([val_metrics, \n",
    "                                    np.expand_dims(get_metrics(val_outputs, val_targets), axis=1)],\n",
    "                                   axis=1)\n",
    "      train_loss = train_loss/len(training_loader)\n",
    "      val_loss = val_loss/len(val_loader)\n",
    "      # Print training/validation statistics \n",
    "      if verbose:\n",
    "        print('Avgerage training loss: {:.6f}\\nAverage validation loss: {:.6f}'.format( \n",
    "                train_loss,\n",
    "                val_loss\n",
    "                ))\n",
    "\n",
    "      # create checkpoint variable and add important data\n",
    "      checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'val_loss_min': val_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "      }\n",
    "      # save checkpoint\n",
    "      best_yet = False\n",
    "      if val_loss <= val_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(val_loss_min,val_loss))\n",
    "        # save checkpoint as best model\n",
    "        best_yet = True\n",
    "        val_loss_min = val_loss\n",
    "      save_checkpoint(checkpoint, best_yet, checkpoint_path, best_model_path)\n",
    "    print('Epoch {}: Done\\n'.format(epoch))\n",
    "  return model, val_metrics\n",
    "\n",
    "# Alternative: just load from disk\n",
    "def run_model(model, loader):\n",
    "  # Switch to evaluation (prediction) mode. Again, this doesn't evaluate anything.\n",
    "  model.eval()\n",
    "\n",
    "  all_targets = np.array([], dtype=int)\n",
    "  all_outputs = np.empty((0, model.output_size), dtype=float)\n",
    "\n",
    "  # We're only testing here, so don't run the backward direction (no_grad).\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(tqdm(loader)):\n",
    "      ids = data['ids'].to(device, dtype = torch.long)\n",
    "      mask = data['mask'].to(device, dtype = torch.long)\n",
    "      targets = data['labels'].int()\n",
    "\n",
    "      outputs = model(ids, mask)\n",
    "\n",
    "      all_targets = np.concatenate([all_targets, targets.numpy()])\n",
    "      all_outputs = np.concatenate([all_outputs, outputs.cpu().detach().numpy()])\n",
    "  return {\n",
    "      'targets': all_targets,\n",
    "      'outputs': all_outputs,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "7512c7bbac2b4a8a86e6759f3bbbc2b8",
      "a36dc39df13c4fb6b06210e9f7d4e5cb",
      "bfa45769c90c4e50829b7bba9c673531",
      "56dbdd9ba90042a1a7ba7075226a6a44",
      "3ed89d90b8d042d2b62f8f24711000df",
      "5359a706b7d04c81b9919471f7f4e275",
      "255c8c0083c040a188e98fea8afed4e0",
      "a356d74e3e1e46d08524c177fdab0c42",
      "d456c732a7934f4780a4e26adebe5d82",
      "64b1d8be1d114100b7ae1cb42bcb3425",
      "ef35c644d0824d529a22bed78aac1ab6",
      "6ee5321a01df4387942623a911ad4245",
      "927b0d1b7f504f11bacd5ae4da1f66b3",
      "0ef258235434491b9d1a585639573073",
      "af533fa5914c4314aa5ff01979ba383d",
      "9807234201dd4c59978e2cb9573a5c9f",
      "ec4577be600d44a69b45cfcfa93d7b1d",
      "7c03819555b54cd981a9d3cdccfbb905",
      "d873ec6fb7b245089656551e517c8336",
      "b71e35f51d5f4495ad5e1468d701097d",
      "4a99e6fc584c4676bcbf18030f687ae3"
     ]
    },
    "id": "DcqgkF06fnkf",
    "outputId": "5d27ba2c-1e57-4d06-d696-790354f3a582"
   },
   "outputs": [],
   "source": [
    "folder_name = 'checkpoints-avg-' + DATASET_NAME\n",
    "!mkdir $folder_name\n",
    "checkpoint_idx_path = \"{}/last_idx\".format(folder_name)\n",
    "\n",
    "# Set up current checkpoint index\n",
    "CHECKPOINT_IDX = 0\n",
    "if os.path.exists(checkpoint_idx_path):\n",
    "    idx_file = open(checkpoint_idx_path, 'r')\n",
    "    CHECKPOINT_IDX = int(idx_file.read()) + 1\n",
    "    idx_file.close()\n",
    "idx_file = open(checkpoint_idx_path, 'w')\n",
    "idx_file.write(str(CHECKPOINT_IDX))\n",
    "idx_file.close()\n",
    "\n",
    "logging.basicConfig(filename=\"{}/{}.log\".format(folder_name, CHECKPOINT_IDX), level=logging.INFO)\n",
    "\n",
    "all_test_metrics = np.zeros((TRAIN_REPEATS, 3), dtype=float)\n",
    "\n",
    "trained_model = None\n",
    "if TRAIN_FROM_SCRATCH:\n",
    "    for i in range(TRAIN_REPEATS):\n",
    "        run_header = \"--- RUN {} ---\".format(i)\n",
    "        print(run_header)\n",
    "        logging.info(run_header)\n",
    "        # Reinitialise weights\n",
    "        model = DistilBERTClass(leaf_count)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Train/validate\n",
    "        CHECKPOINT_PATH = './{}/{}_{}_current.pt'.format(folder_name, CHECKPOINT_IDX, i)\n",
    "        BEST_CHECKPOINT_PATH = './{}/{}_{}_best.pt'.format(folder_name, CHECKPOINT_IDX, i)\n",
    "        trained_model, val_metrics = train_model(\n",
    "            1, # Count from one\n",
    "            EPOCHS, # EPOCH passes over our training set\n",
    "            np.Inf, \n",
    "            train_loader, \n",
    "            val_loader, \n",
    "            model, \n",
    "            CHECKPOINT_PATH,\n",
    "            BEST_CHECKPOINT_PATH,\n",
    "        )          \n",
    "        # Test\n",
    "        test_result = run_model(trained_model, test_loader)\n",
    "        test_metrics = get_metrics(test_result['outputs'], test_result['targets'], True, True)\n",
    "        all_test_metrics[i, :] = test_metrics\n",
    "    averaged = np.average(all_test_metrics, axis = 0)\n",
    "    averaged_display = '--- Average of {} runs:\\nLeaf accuracy: {}\\nLeaf precision: {}\\nLeaf AU(PRC): {}'.format(\n",
    "        TRAIN_REPEATS, averaged[0], averaged[1], averaged[2])\n",
    "    print(averaged_display)\n",
    "    logging.info(averaged_display)\n",
    "else:\n",
    "    load_path = '{}/{}_{}_{}.pt'.format(folder_name, LOAD_ITERATION, TRAIN_REPEATS - 1, 'best' if LOAD_BEST else 'current')\n",
    "    trained_model, _ = load_checkpoint(load_path, (encoder, classifier))\n",
    "    # Test\n",
    "    test_result = run_model(trained_model, test_loader)\n",
    "    get_metrics(test_result['outputs'], test_result['targets'], True, True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DB-Linear-old-avg.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "255c8c0083c040a188e98fea8afed4e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ed89d90b8d042d2b62f8f24711000df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef35c644d0824d529a22bed78aac1ab6",
      "placeholder": "​",
      "style": "IPY_MODEL_64b1d8be1d114100b7ae1cb42bcb3425",
      "value": " 120406/120406 [2:07:24&lt;00:00, 15.76it/s]"
     }
    },
    "5359a706b7d04c81b9919471f7f4e275": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "56dbdd9ba90042a1a7ba7075226a6a44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d456c732a7934f4780a4e26adebe5d82",
      "max": 120406,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a356d74e3e1e46d08524c177fdab0c42",
      "value": 120406
     }
    },
    "64b1d8be1d114100b7ae1cb42bcb3425": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7512c7bbac2b4a8a86e6759f3bbbc2b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bfa45769c90c4e50829b7bba9c673531",
       "IPY_MODEL_56dbdd9ba90042a1a7ba7075226a6a44",
       "IPY_MODEL_3ed89d90b8d042d2b62f8f24711000df"
      ],
      "layout": "IPY_MODEL_a36dc39df13c4fb6b06210e9f7d4e5cb"
     }
    },
    "a356d74e3e1e46d08524c177fdab0c42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a36dc39df13c4fb6b06210e9f7d4e5cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bfa45769c90c4e50829b7bba9c673531": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_255c8c0083c040a188e98fea8afed4e0",
      "placeholder": "​",
      "style": "IPY_MODEL_5359a706b7d04c81b9919471f7f4e275",
      "value": "100%"
     }
    },
    "d456c732a7934f4780a4e26adebe5d82": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef35c644d0824d529a22bed78aac1ab6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
