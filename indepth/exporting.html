<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Exporting your models &mdash; KTT Hierarchical Classification System 0.3a documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Encoders" href="../encoders/index.html" />
    <link rel="prev" title="Training stage" href="training.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> KTT Hierarchical Classification System
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../usage/index.html">Usage</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../usage/installation.html">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../usage/installation.html#downloading">Downloading</a></li>
<li class="toctree-l3"><a class="reference internal" href="../usage/installation.html#setting-up-dependencies">Setting up dependencies</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../usage/quickstart.html">Quickstart</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../usage/quickstart.html#data-preparation">Data preparation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../usage/quickstart.html#training-a-model">Training a model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../usage/quickstart.html#exporting-the-trained-model">Exporting the trained model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../usage/quickstart.html#serving-up-a-bento">Serving up a Bento</a></li>
<li class="toctree-l3"><a class="reference internal" href="../usage/quickstart.html#shipping-bentos-in-a-container">Shipping Bentos in a container</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../usage/commands.html">CLI usage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../usage/commands.html#adapters">Adapters</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">System design</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="adapters.html">Data adapters</a><ul>
<li class="toctree-l3"><a class="reference internal" href="adapters.html#intermediate-format-specification">Intermediate format specification</a><ul>
<li class="toctree-l4"><a class="reference internal" href="adapters.html#parquet-schema">Parquet schema</a></li>
<li class="toctree-l4"><a class="reference internal" href="adapters.html#hierarchy-json-schema">Hierarchy JSON schema</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="adapters.html#theory">Theory</a><ul>
<li class="toctree-l4"><a class="reference internal" href="adapters.html#the-sql-adapter">The SQL adapter</a></li>
<li class="toctree-l4"><a class="reference internal" href="adapters.html#the-flatfile-adapter">The flatfile adapter</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="training.html">Training stage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="training.html#the-process">The process</a></li>
<li class="toctree-l3"><a class="reference internal" href="training.html#classes">Classes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="training.html#common-classes">Common classes</a></li>
<li class="toctree-l4"><a class="reference internal" href="training.html#pytorch-utilities">PyTorch utilities</a></li>
<li class="toctree-l4"><a class="reference internal" href="training.html#scikit-learn-utilities">Scikit-learn utilities</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Exporting your models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#onnx-exporting">ONNX exporting</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bentoml-exporting">BentoML exporting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#packaging">Packaging</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../encoders/index.html">Encoders</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../encoders/distilbert.html">DistilBERT</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../encoders/distilbert.html#api">API</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../encoders/sklearn_text.html">Scikit-learn text feature extractors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../encoders/sklearn_text.html#api">API</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../models/index.html">Prebuilt models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../models/tfidf_lsgd.html">Tf-idf + Leaf SGD</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../models/tfidf_lsgd.html#api">API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/tfidf_lsgd.html#configuration-schema">Configuration schema</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/tfidf_lsgd.html#default-tuning-configuration">Default tuning configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/tfidf_lsgd.html#theory">Theory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../models/tfidf_hsgd.html">Tf-idf + Hierarchy SGD</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../models/tfidf_hsgd.html#api">API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/tfidf_hsgd.html#configuration-schema">Configuration schema</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/tfidf_hsgd.html#default-tuning-configuration">Default tuning configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/tfidf_hsgd.html#theory">Theory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../models/db_bhcn.html">DB-BHCN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../models/db_bhcn.html#api">API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/db_bhcn.html#configuration-schema">Configuration schema</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/db_bhcn.html#default-tuning-configuration">Default tuning configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/db_bhcn.html#checkpoint-schema">Checkpoint schema</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/db_bhcn.html#theory">Theory</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../models/db_bhcn.html#id1">DB-BHCN</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../models/db_bhcn_awx.html">DB-BHCN+AWX</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../models/db_bhcn_awx.html#api">API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/db_bhcn_awx.html#configuration-schema">Configuration schema</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/db_bhcn_awx.html#default-tuning-configuration">Default tuning configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/db_bhcn_awx.html#checkpoint-schema">Checkpoint schema</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/db_bhcn_awx.html#theory">Theory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../models/db_ahmcnf.html">DistilBERT + Adapted HMCN-F</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../models/db_ahmcnf.html#api">API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/db_ahmcnf.html#configuration-schema">Configuration schema</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/db_ahmcnf.html#default-tuning-configuration">Default tuning configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/db_ahmcnf.html#checkpoint-schema">Checkpoint schema</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/db_ahmcnf.html#theory">Theory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../models/db_achmcnn.html">DistilBERT + Adapted C-HMCNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../models/db_achmcnn.html#api">API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/db_achmcnn.html#configuration-schema">Configuration schema</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/db_achmcnn.html#default-tuning-configuration">Default tuning configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/db_achmcnn.html#checkpoint-schema">Checkpoint schema</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/db_achmcnn.html#theory">Theory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../models/db_linear.html">DistilBERT + Linear</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../models/db_linear.html#api">API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/db_linear.html#configuration-schema">Configuration schema</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/db_linear.html#checkpoint-schema">Checkpoint schema</a></li>
<li class="toctree-l3"><a class="reference internal" href="../models/db_linear.html#theory">Theory</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../dev-encoder/index.html">Developing new encoders</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../dev-encoder/index.html#where-encoders-come-in">Where encoders come in</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dev-encoder/index.html#adding-encoders">Adding encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dev-encoder/index.html#implementing-preprocessors">Implementing preprocessors</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../dev/intro.html">Developing new models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../dev/intro.html#frameworks">Frameworks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dev/intro.html#general-model-folder-structure">General model folder structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dev/intro.html#the-model-itself">The model itself</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dev/intro.html#checkpointing">Checkpointing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dev/intro.html#preprocessing-needs">Preprocessing needs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dev/intro.html#exporting">Exporting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../dev/intro.html#onnx">ONNX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../dev/intro.html#export-bento-resources"><code class="docutils literal notranslate"><span class="pre">export_bento_resources</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../dev/intro.html#the-service-implementation">The service implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../dev/intro.html#the-service-configuration-files">The service configuration files</a></li>
<li class="toctree-l4"><a class="reference internal" href="../dev/intro.html#the-reference-dataset">The reference dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../dev/intro.html#the-export-bento-resources-method">The <code class="docutils literal notranslate"><span class="pre">export_bento_resources</span></code> method</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../dev/intro.html#specifying-your-hyperparameters-optional">Specifying your hyperparameters (optional)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dev/intro.html#registering-your-model-with-the-rest-of-the-system">Registering your model with the rest of the system</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../dev/intro.html#the-model-lists">The model lists</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../dev/intro.html#test-run-your-model">Test-run your model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../dev/intro.html#grafana-dashboard-design-optional">Grafana dashboard design (optional)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../dev/intro.html#testing-automatic-dashboard-provisioning">Testing automatic dashboard provisioning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../dev/intro.html#framework-specific-guides">Framework-specific guides</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../dev/pytorch/add_model.html">Implementing a model with PyTorch+DistilBERT</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../dev/pytorch/add_model.html#the-model">The model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../dev/pytorch/add_model.html#pytorch-model-module-structure">PyTorch model module structure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../dev/pytorch/add_model.html#pytorch-utilities">PyTorch utilities</a></li>
<li class="toctree-l4"><a class="reference internal" href="../dev/pytorch/add_model.html#the-process">The process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../dev/pytorch/add_model.html#registering-testing-conclusion">Registering, testing &amp; conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../dev/sklearn/add_model.html">Implementing a model with Scikit-learn</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../dev/sklearn/add_model.html#the-model">The model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../dev/sklearn/add_model.html#scikit-learn-utilities">Scikit-learn utilities</a></li>
<li class="toctree-l4"><a class="reference internal" href="../dev/sklearn/add_model.html#the-process">The process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../dev/sklearn/add_model.html#registering-testing-conclusion">Registering, testing &amp; conclusion</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/index.html">Advanced guides</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../advanced/dvc.html">Using DVC with our system</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/gpu.html">Inferencing with GPUs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../advanced/gpu.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/gpu.html#gpu-based-inference-using-bentos">GPU-based inference using Bentos</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/gpu.html#gpu-based-inference-for-dockerised-services">GPU-based inference for Dockerised services</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../advanced/gpu.html#without-monitoring-capabilities">Without monitoring capabilities</a></li>
<li class="toctree-l4"><a class="reference internal" href="../advanced/gpu.html#with-monitoring-capabilities">With monitoring capabilities</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/tuning.html">Automatic hyperparameter tuning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../advanced/tuning.html#cli-usage">CLI usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/tuning.html#tune-configuration-format">Tune configuration format</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../ref.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">KTT Hierarchical Classification System</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="index.html">System design</a> &raquo;</li>
      <li>Exporting your models</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/indepth/exporting.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="exporting-your-models">
<h1>Exporting your models<a class="headerlink" href="#exporting-your-models" title="Permalink to this heading"></a></h1>
<p>KTT is not an inference system - it produces models and additionally generates inference systems based around those models for you. KTT supports exporting trained models in two ways:</p>
<ul class="simple">
<li><p>Open Neural Network Exchange (ONNX) format - an open, vendor-neutral format that can be deployed to almost everywhere.</p></li>
<li><p>BentoML - which internally also partly uses ONNX to store models. This is to be used with our bundled REST API-powered inference system.</p></li>
</ul>
<section id="onnx-exporting">
<h2>ONNX exporting<a class="headerlink" href="#onnx-exporting" title="Permalink to this heading"></a></h2>
<p>This is the way to go if you intend to use KTT-generated models with an existing inference service, such as Azure ML, or if you already have your own framework set up and just need the models themselves.
All bundled models in KTT except one (<a class="reference internal" href="../models/tfidf_hsgd.html"><span class="doc">Tf-idf + Hierarchy SGD</span></a>) supports exporting to ONNX. The exact file structure is not enforced to allow model implementers more freedom in how they export their designs, but bundled PyTorch models generally follow a two-graph scheme:</p>
<ul class="simple">
<li><p>The encoder (currently DistilBERT is bundled) is exported to an ONNX graph in <code class="docutils literal notranslate"><span class="pre">outputs/modelname_datasetname/encoder</span></code> in several different files based on their own exporting logic.</p></li>
<li><p>The classifier head (any of the five PyTorch models) is exported to a single file, <code class="docutils literal notranslate"><span class="pre">outputs/modelname_datasetname/classifier/classifier.onnx</span></code>.</p></li>
</ul>
<p>Currently, KTT exports to ONNX using opset version 11. All ONNX models support dynamic minibatch axes in that they can be passed one or many data items at once. This is especially useful should you want to use the exported models for minibatched offline inference, or dynamically microbatched real-time inference.</p>
</section>
<section id="bentoml-exporting">
<h2>BentoML exporting<a class="headerlink" href="#bentoml-exporting" title="Permalink to this heading"></a></h2>
<p>KTT is able to construct BentoServices from its models based on resources defined by the models themselves. A BentoService can be constructed either with or without a production-time performance monitoring system. When built with such system integrated, a deployment would look something like this out-of-the-box:</p>
<a class="reference internal image-reference" href="../_images/bentoml-deploy.svg"><img alt="A full deployment of a BentoService on a single host." src="../_images/bentoml-deploy.svg" width="800" /></a>
<p>A full BentoService deployment consists of four smaller services linked together in a Docker network: <code class="docutils literal notranslate"><span class="pre">inference</span></code>, <code class="docutils literal notranslate"><span class="pre">monitoring</span></code>, <code class="docutils literal notranslate"><span class="pre">prometheus</span></code> and <code class="docutils literal notranslate"><span class="pre">grafana</span></code>. The main workings of the deployed service can be described as follows:</p>
<ol class="arabic simple">
<li><p>User sends a JSON request to the inference service. It produces an answer, respond to the user, and log both the user input (in the form of feature vectors - not raw text) and its own answers (in the form of label scores) to the monitoring service. A background process of the inference service also keeps track of response time and other system performance metrics.</p></li>
<li><p>The monitoring service accumulates data received from the inference service into a rolling dataset of N newest rows. This dataset is periodically compared against a <em>reference dataset</em> produced during training of the model this service uses, and the results are stored temporarily locally.</p></li>
<li><p>The Prometheus service, which as its name implies is a Prometheus time-series database instance, periodically fetches metrics from both the inference and monitoring services.</p></li>
<li><p>The Grafana dashboard periodically fetches the latest data stored within Prometheus, transforms them, and visualises them as a user-friendly performance monitoring dashboard.</p></li>
</ol>
<p>By default, all four services communicate to each other via a Docker network, and all are directly accessible from the host machine via the same port number. With a bit of modification to the <code class="docutils literal notranslate"><span class="pre">docker-compose.yaml</span></code> preset produced by KTT, one can also host these four services separately if desired.</p>
<p>Currently, BentoML services do not make use of ONNX graphs due to BentoML LTS’s rather buggy nature with the ONNX + CUDA runtime combination. As such, they directly use serialised versions of the models as implemented by BentoML.</p>
<section id="packaging">
<h3>Packaging<a class="headerlink" href="#packaging" title="Permalink to this heading"></a></h3>
<p>KTT-produced BentoServices are easily packaged - they are self-contained within their own directory in the <code class="docutils literal notranslate"><span class="pre">build</span></code> folder. Sinply zip them up (or use any archival format you like) and you have a portable system.</p>
<p>Advanced users might want to produce their own Docker Compose configurations, or use Kubernetes instead.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="training.html" class="btn btn-neutral float-left" title="Training stage" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../encoders/index.html" class="btn btn-neutral float-right" title="Encoders" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Huynh Thien Khiem, Voong Xay Tac, Huynh Truong Tu.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>